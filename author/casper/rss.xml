<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>airfer.github.io/</title>
   
   <link>http://airfer.github.io/</link>
   <description>There are some things in this world will never change and some things do change,everything that has a beginning has an end </description>
   <language>en-uk</language>
   <managingEditor> Wang Yukun</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>侵入式核心链路探测工具Rattler</title>
	  <link>//opensource-rattler</link>
	  <author>Wang Yukun</author>
	  <pubDate>2019-09-10T19:28:00+00:00</pubDate>
	  <guid>//opensource-rattler</guid>
	  <description><![CDATA[
	     <p><a href="LICENSE"><img src="https://img.shields.io/badge/License-GPL%203.0-red.svg" alt="License" /></a> <a href="https://travis-ci.com/airfer/rattler"><img src="https://api.travis-ci.com/airfer/rattler.svg?branch=master" alt="Build Status" /></a> <a href="https://github.com/airfer/rattler/issues"><img src="https://img.shields.io/github/issues/airfer/rattler.svg" alt="GitHub issues" /></a> <a href="https://codecov.io/gh/airfer/rattler"><img src="https://codecov.io/gh/airfer/rattler/branch/master/graph/badge.svg" alt="codecov" /></a></p>

<h2 id="section">项目简介</h2>

<p>每个服务都有属于自身的核心特性。我们将服务的核心特性进行抽象，并使用核心链路这个概念来具象化核心特性。说的更直白一些，核心链路覆盖了核心服务绝大部分功能(超过80%)，例如某服务为交易服务，那么下单链路、付款链路、退单链路、退款链路都属于核心链路。</p>

<p>在进行迭代开发的过程中，新增或者修改的代码对核心链路的影响一直都有RD评估后给出，缺乏量化的标准。比如现在对交易服务中的某个公共service做了修改，那么这个service的变动会影响那些核心链路呢？这部分目前没有数据支撑。</p>

<p>rattler是一个侵入式的核心链路信息收集工具，通过CoreChainClass注解以及CoreChainMethod注解来收集服务的链路信息，然后将链路信息上送到指定的服务器，目前仅支持http上送。</p>

<p>代码的变更最后反馈到核心链路的变更，这样就有了一个量化的数据来衡量本次需求对原有核心链路的影响.</p>

<blockquote>
  <p><strong>需要注意的是：这里核心链路的概念并非指的是多个微服务之间通过HTTP或者RPC组成的调用链路，而是指在单个微服务本身中函数之间的链路调用关系。本工具更加适用于偏底层的核心服务，比如支付的记账管理、搜索引擎的广告检索、金融对账结算等类似场景。</strong></p>
</blockquote>

<h2 id="section-1">链路信息采集方式对比</h2>

<p>目前链路采集方式，目前主要有两种方式：一种手动采集、手动记录，以文本文档的方式存储（xml,yml,xmind,xls），一种为注解方式，现在就主要的收集方式做一下对比:</p>
<table>
    <tr>
        <td>记录方式</td>
        <td>优势</td>
        <td>不足</td>
    </tr> 
    <tr>
        <td>
            以注解形式内嵌在代码中
        </td>
        <td>
            <ul>
                <li>链路信息维护以及更新简单、更新实效性高</li>             
                <li>RD可无障碍参与核心链路信息建设</li>             
                <li>提供更细维度支持，比如方法的权重</li>
            </ul>
        </td>
        <td>
            <ul>            
                <li>需要对代码进行改造</li>       
                <li>链路信息不直观</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td>
            文本文档的方式存储（xml,yml,xmind,etc）
        </td>
        <td>
            <ul>
                <li>链路调用较为直观</li>     
                <li>无需代码层面的改造</li>
            </ul>                    
        </td>
        <td>
            <ul>
                <li>链路信息的维护以及更新难度大</li>     
                <li>推动RD参与建设难度较大</li>        
                <li>无法更细维度进行统计</li>
            </ul>
        </td>
    </tr>
</table>
<p>由于需求的快速迭代，核心链路信息的维护成本将会逐渐变高，所以优先考虑对核心链路信息的维护和更新。当前以注解的形式进行信息收集更为妥当。当然如果服务较为稳定，核心链路的变更较少，那么可以采用文本的方式记录，具体方式可根据实际情况以及自身业务特点进行灵活选择。</p>

<h2 id="section-2">核心原理</h2>

<p>1、 定义CoreChainClass、CoreChainMethod注解，使用注解在服务工程中进行标注(侵入式)</p>

<p>2、 利用Reflections的反射机制，静态扫描指定package的标注类或者标注方法</p>

<p>3、 将扫描得到的核心链路数据上送到指定的服务，目前仅支持HTTP</p>

<h2 id="section-3">使用方法</h2>

<p>由于rattler进行链路信息采集，所以需要在项目中集成该项目SDK，通过jar包引入。</p>

<h3 id="java">java包引入</h3>

<p>在使用之前需要事先通过pom引入jar文件，可以使用本开源的jar包，也可以自行编译上传到私有maven仓库，示例：</p>

<pre><code class="xml">
<dependency>
    <groupid>com.github.airfer</groupid>
    <artifactid>rattler</artifactid>
    <version>1.0.0-SNAPSHOT</version>
</dependency>
</code></pre>
<p>### 注解引入</p>

<p>注解引入，有两种引入方式。可以通过CoreChainClass注解和CoreChainMethod注解引入。由于目前是侵入式收集，会直接修改代码，所以最好和RD一起来做这个事情。
由于链路收集的收集采用静态扫描方式，所以理论上不会对注入服务造成性能影响。为了避免对线上服务造成潜在影响，可以在线上环境可以关闭链路信息收集以及信息上送服务，如何关闭请参见配置说明部分。</p>

<blockquote>
  <p>由于<a href="/src/main/java/com/airfer/rattler/aspect/CoreChain.java">CoreChain</a>通过Component引入，所以在注解引入后需要在ComponentScan配置中添加rattler所在的包名，示例如下所示。如果对本项目重新打包并变更包名则可省去扫描包添加的步骤。当前对注解标注的权重信息留在下一期支持</p>
</blockquote>

<ul>
  <li>CoreChainClass注解引入</li>
</ul>
<pre><code class="java">
@CoreChainClass(coreChainName="查询链路",weightEnum=MethodWeightEnum.LOW)
public class QueryService{
   /**
   * 通过Class方式引入，所有在class中的方法都将被标注相同的链路名称，以及链路权重
   */ 
}
</code></pre>

<ul>
  <li>CoreChainMethod注解引入</li>
</ul>
<pre><code class="java">
@Serivice
public class QueryService{
   /**
   * 通过Method方式引入，只有该Method会被标注，其他方法不会被标注
   */ 
   @Autowired
   private CommonService commonService;
   
   @CoreChainMethod(coreChainName="查询链路",weightEnum=MethodWeightEnum.LOW)
   private String queryOrderNum(){
       try{
           return commonService.queryOrderInfo();
       }catch (RuntimeException re){
           log.error("runtime error");
       }
   }
}
</code></pre>

<ul>
  <li>ComponentScan扫描包添加</li>
</ul>

<pre><code class="java">
@SpringBootApplication(exclude = {DataSourceAutoConfiguration.class,MongoAutoConfiguration.class,MongoAutoConfiguration.class, MongoDataAutoConfiguration.class})
@ComponentScan(basePackages = {"com.airfer.rattler"})
public class Application {
}
</code></pre>

<h2 id="section-4">配置文件使用</h2>

<p>服务引入jar包，并添加链路注解后，需要添加rattler配置文件。添加方式为在resource目录下新增rattler.properties文件，内容示例如下：</p>
<pre><code class="text">
#链路上报开关，true开启，false关闭。在线上环境可以选择关闭【必填】
chain_capture_switch=true

#链路信息的上送接口，采用post方式进行上送【选填】|【链路开关打开必填】
#upload_url=-1 表示不进行上送
upload_url=http://upload.domain.info.com/chain/test

#待扫描的服务package名称，目前不支持多package配置【必填】|【链路开关打开必填】
package_name=com.airfer.rattler.data

#在链路开关开启且后端服务启动后，rattler会按照指定的刷新频率刷新收集，单位秒【选填】|【链路开关打开必填】
# refresh_interval=-1 表示不启动定时刷新
refresh_interval=-1

#server_id唯一标识一个服务，和美团的appkey是同一个概念【选填】|【链路开关打开必填】
server_id=airfer_rattler_test
</code></pre>

<h2 id="rattler-func-detection">rattler-func-detection</h2>

<p>rattler-func-detetion 是一个基于Git Patch文件的开源代码分析工具(仅限java服务)，主要用于函数变动探测，目前支持python2.x以及python3.x版本。通过以下命令：</p>
<pre><code class="bash">
git diff feature/* master &gt;result.patch
</code></pre>
<p>获取patch文件后，解析patch文件，获取变动的函数列表，并将函数列表信息上送到指定服务器，目前仅支持Http协议</p>

<h3 id="section-5">主要用途</h3>
<ul>
  <li>基于git patch文件分析函数变动，进行服务核心链路冲撞率(collision rate)计算，评估影响面大小</li>
  <li>统计核心链路的变更频率，为服务的重构以及优化提供数据支持</li>
</ul>

<h3 id="section-6">安装方法</h3>

<ol>
  <li>下载源代码，使用python setup.py install进行安装</li>
  <li>使用rattler-func-detection 命令运行,可通过 –help命令查看可选项</li>
</ol>

<h3 id="section-7">命令行信息详解</h3>
<pre><code class="text">
Usage: rattler-func-detection [OPTIONS]

  :param rootPath: 扫描根路径 :param diffFilePath: :return:

Options:
  --root_path  TEXT  扫描根路径,git clone代码后根目录
  --diff_path  TEXT  git diff文件路径
  --server_id  TEXT  server_id,服务唯一标识
  --upload_url TEXT  数据上送地址
  --help             Show this message and exit.
</code></pre>

<h2 id="section-8">链路碰撞分析</h2>

<p>假设我们现在有两个核心链路A和A1(<strong><em>单个服务内部函数之间的链路调用关系，非微服务之间的调用，A-G、A1-G1皆为函数</em></strong>),两条链路中的所有调用函数都可通过CoreChain类注解进行收集。而需求迭代所造成的代码变动可通过rattler-func-detection进行收集，链路碰撞示意图如下所示：
<img src="https://airfer.github.io/images/rattler/func-detection.jpg" alt="detect" /></p>

<h2 id="section-9">结果预览</h2>
<p>拿到链路数据之后，就可以计算每条链路中有多少函数处于被影响的范围。虽然在进行链路信息标注时设置了权重值，但是目前权重值还没有被应用于计算，预计下期项目提供支持</p>
<pre><code class="text">
{
    u'data': u'{
        "all": "0.36", // 整体冲撞率
        "A"  : "0.57", // 消费链路冲撞率
        "A1" : "0.23"  // 开户链路冲撞率
    }',
    u'resCode': u'00'
}
</code></pre>

<h2 id="section-10">使用限制</h2>

<ul>
  <li>rattler基于JDK8开发，使用了很多新特性，比如stream，对于基于低于JDK8的服务版本暂不支持</li>
  <li>目前rattler仅适用于JAVA后端服务，对于非JAVA服务以及前端暂不支持</li>
</ul>

<h2 id="section-11">排期计划</h2>

<ul>
  <li>方法权重值纳入链路冲撞分析范围 预计2019/11</li>
</ul>


	  ]]></description>
	</item>

	<item>
	  <title>Code Review实施规范V2.0</title>
	  <link>//codeDiff2.0</link>
	  <author>Wang Yukun</author>
	  <pubDate>2019-04-04T16:28:00+00:00</pubDate>
	  <guid>//codeDiff2.0</guid>
	  <description><![CDATA[
	     <p>
    Code Review作为质量保障的重要一环，在很长时间内没有引起足够的重视，在趟过了很多坑后发现，有很多的错误在Code Diff阶段能够很好的避免。
    文中总结了Code Review过程中曾遇到的问题，并形成规范，尽可能在Review阶段发现尽可能多的bug，做好质量保障！
</p>
<embed src="/pdfs/codeDiff.pdf" type="application/pdf" />


	  ]]></description>
	</item>

	<item>
	  <title>Code Review实施规范V1.0草案</title>
	  <link>//Codediff2019</link>
	  <author>Wang Yukun</author>
	  <pubDate>2019-03-04T16:28:00+00:00</pubDate>
	  <guid>//Codediff2019</guid>
	  <description><![CDATA[
	     <p>
    Code Review作为质量保障的重要一环，在很长时间内没有引起足够的重视，在趟过了很多坑后发现，有很多的错误在Code Diff阶段能够很好的避免。
    文中总结了Code Review过程中曾遇到的问题，并形成规范，尽可能在Review阶段发现尽可能多的bug，做好质量保障！
</p>
<p><img src="/images/mt/codediff/codediff-1.png" alt="" />
<img src="/images/mt/codediff/codediff-2.png" alt="" />
<img src="/images/mt/codediff/codediff-3.png" alt="" />
<img src="/images/mt/codediff/codediff-4.png" alt="" />
<img src="/images/mt/codediff/codediff-5.png" alt="" /></p>


	  ]]></description>
	</item>

	<item>
	  <title>2017-2018年度测试总结</title>
	  <link>//summerize2018</link>
	  <author>Wang Yukun</author>
	  <pubDate>2018-02-20T19:28:00+00:00</pubDate>
	  <guid>//summerize2018</guid>
	  <description><![CDATA[
	     <p>
    2017-2018年算是成长比较快的一年，在业务测试、流程推进、自动化测试、持续集成、平台开发、规范制定等方面都有了一些别样的收获。
    2018年已经远去，记录一下2017-2018年的自己，既往不恋，纵情向前！
</p>

<p><img src="/images/mt/summerize/summerize_mt.002.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.003.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.004.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.005.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.006.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.007.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.008.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.009.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.0010.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.011.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.012.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.013.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.014.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.015.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.016.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.017.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.018.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.019.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.020.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.021.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.022.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.023.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.024.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.025.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.026.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.027.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.027.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.029.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.030.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.031.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.032.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.033.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.034.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.035.jpeg" alt="" /></p>


	  ]]></description>
	</item>

	<item>
	  <title>kubernetes在测试中的应用实践</title>
	  <link>//k8s-practice</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-08-29T19:28:00+00:00</pubDate>
	  <guid>//k8s-practice</guid>
	  <description><![CDATA[
	     <h4>前言</h4>
<p>在休假之前就想写一写kubernetes在测试中应用的一些事，后来因为各种原因被耽搁了下来。这次趁着休假期间的空闲，把应用方面的东西梳理一下，算做是对之前一段研究东西的总结。</p>

<p>研究kubernetes也有一段时间了，在测试方面的应用我将其归类为三个方面：分别为服务部署、环境搭建、持续集成优化 。其实每个方面都值得写一篇文章进行详细阐述，只是最近变的有些懒，就写个总的概况吧。对这个方向感兴趣的同仁欢迎拍砖，也欢迎大家留言探讨，正餐现在开始。</p>

<h4>服务部署</h4>
<p>服务部署作为k8s最主要的核心功能，在实践中被广泛应用是一件很正常的事情。其实这里讲的服务部署，主要是测试服务的部署，当然这和其他服务的部署没有本质的区别。这样做的目的是充分利用k8s集群的特性为我们服务，保证我们测试服务稳定且高效。目前我们的k8s集群上部署了两个服务，一个是jenkins，一个是镜像生成服务takara。就以takara为例来举例说明吧。</p>

<p>首先介绍一下takara，takara服务是根据用户提供的dockerfile文件生成image镜像，并将镜像推送到本地私有registry，成功后返回生成的镜像信息供后续部署使用。其工作原理如下图所示：
<img src="/images/k8s/takara.jpg" alt="" /></p>

<p>takara作为基础服务存在，如果其出现问题将影响后续所有的部署、构建过程，所以其必须稳定。后续由于多个业务线会使用该服务，为了高效快速的生成镜像减少排队情况的发生，Image 生成器会有多个。</p>

<p>从图中可以看出，HttpServer接受get/post的请求，请求的信息中包含dockerfile的地址信息、所需的依赖文件信息等。关于所依赖的物料数据来源有多种方式，例如使用rsync工具，或者直接从hadoop上获取。HttpServer的作用在于解析请求数据，并将解析后的数据写入任务调度队列，供后续worker使用。</p>

<p>由于worker生产镜像的过程是无状态的，所以可以通过kubernetes的replica set动态调整worker的数目。每一个worker位于一个pod中，由两个container组成，分别为Prepare container和Generate container。</p>

<p>前者用于docker build之前的准备工作，主要是获取dockerfile文件，以及相关的依赖文件，并将获得的文件数据进行持久化存储。当前持久化存储使用的是nfs，目前在测试环境下支持良好，速度较快。后者用于根据获取的dockerfile文件数据，生成image镜像，并将镜像重新命名，打tag后推送到私有的镜像仓库，供后续使用。</p>

<p>由于httpserver的上层是kubernetes的service，所以即使一个httpserver挂掉，也不会影响整体的运行。httpserver的数量以及worker的数量都通过replica set进行控制。可以发现，使用kubernetes进行服务部署的好处在于服务的稳定高效，无状态扩展简单。并不是所有的服务都适合在kubernetes上部署，具体情况需要具体分析。
环境搭建
在测试过程中，环境搭建是一个比较头疼的事情，各种操作系统的版本，各种依赖库，每一次搭建都要重复一遍这个痛苦的过程。那么有没有一种方法可以一劳永逸呢？其实答案就是镜像。我们将各种操作系统的版本以及服务运行所依赖的lib库生成一个镜像，然后将镜像上传到我们的私有registry，每次在使用的时候直接从registry中拖取就可以了。</p>

<p>这样看来是非常简单的事情，那么这和kubernetes有什么关系呢?我们安装docker直接pull不可以吗？其实在实际应用中会发现遇到的问题比刚开始所想的要多的多。那么考虑下面几个方面：</p>

<ul>
  <li>如果开发同学想要一个环境，但是没有空闲的机器</li>
  <li>如果你在本地环境发现某问题，如何让开发快速的定位修复</li>
  <li>如何和同组的其他同学分享你的测试环境</li>
  <li>当测试环境很多时，如何有效的管理这些环境</li>
  <li>当测试环境由于一些因素挂掉时，如何保证稳定可用</li>
</ul>

<p>通过上述的问题，我们发现要满足上面的几个方面，需要保证以下几点：</p>

<ul>
  <li>非初次环境的部署要快速，不能部署一个环境需要半天甚至更长的时间</li>
  <li>环境需要有独立的ip，可以通过ssh访问，便于开发定位问题</li>
  <li>当环境很多时，需要有一个列表或者table列出环境与ip的对应关系，方便查找</li>
  <li>当环境因某些原因挂掉时，可以自行重启恢复，重启后数据不丢失</li>
</ul>

<p>目前部门内部试用的环境部署方案如下图所示：
<img src="/images/k8s/ssh.jpg" alt="" /></p>

<p>从图中可以看出，整体的结构比较简单，其实环境部署的重点在于镜像制作。用户通过ssh登录到中控机，然后在中控机上通过ssh免密码登录所需服务。但是这里需要对以下几点进行说明：</p>

<ul>
  <li>由于集群内部中pod的ip为weave overlay网段私有ip地址，所以在公司内网中是无法直接访问。为了解决这个问题，一种方式是将镜像中的sshd服务通过service开放出来，这样通过访问service可以访问sshd服务，但是这种方式存在一些问题，比如在内网中存在external ip无法获取的情况</li>
  <li>另一种方式，将kubernetes集群中的一个node作为中控节点，通过在中控节点访问kubernetes内部的pod，目前部门内部采用的是这种方式。这种方式相比第一种方法，比较简单不需要配置service，同时也起到了一定的安全保护作用（只可从中控机访问）</li>
  <li>每个需要启动的环境镜像，都包含了中控机的ssh公钥信息，这样可以做到免密码登录</li>
  <li>至于查看环境与ip的对应关系，通过kubernetes的dashboard或者在master节点上查看都可以，不过推荐dashboard</li>
</ul>

<h4>持续集成优化</h4>
<p>持续集成一直是测试关注的重点，在持续集成的实践中，我们也碰到了一些问题。比如，我们需要选择具体在某个jenkins slave node中执行我们的程序，由于程序的执行对操作系统以及相关的依赖库都有要求，所以在运行jenkins任务之前，我们必须保证slave节点已经配置了任务要求所有的要素。</p>

<p>每新增一个slave节点，上述的环境配置都需要重新配置，很耗费时间。同时如果我们的slave节点挂掉了，我们需要去重新启动这个节点；如果这个节点已经下线，那么就需要重新申请一个节点，或者将该任务的执行节点重新迁移到新的节点。</p>

<p>当任务不多，并且节点变动不是很频繁时，这个需求并不是很强烈。但是当支持的业务线众多，各服务依赖的操作系统以及相关库差异性很大时，基于容器的持续集成就变得优势巨大。每个业务线都可以根据自己的需求定制自己的镜像，持续部署时依赖自己的镜像就可以了。</p>

<p>举个例子，假设有5台slave机子都安装了某依赖库，后来由于服务升级，该依赖库也需要进行升级。如果按照传统的方法，需要分别到这5台机子上进行升级，而如果使用镜像，只需要重新生成镜像即可。</p>

<p>使用kubernetes集群进行持续集成最大的优点就是解决了环境依赖问题，最终这个任务究竟在哪个node，哪个pod中运行，根本无需担心。目前基于kubernetes集群的持续集成有两个方式：
一种是直接在master节点上按照pipeline的stage执行程序，程序直接在master节点上执行，和服务部署相同，程序运行结束，pod就终止被删除。
另一种方式，是利用jenkins的kubernetes插件，关于其使用说明github中的文档已经写的很清楚了，就不再赘述了。</p>

<p>关于pipeline的使用并不是本篇讨论的重点，后续@宋大神会专门写一篇文章介绍。</p>

<p>这里以目前的竞价服务的pipeline进行简要的说明。对于竞价服务，和其他服务一样，我们需要一个完整的持续集成体系。在这个持续集成的pipeline中，包含代码静态检测、分布式功能性测试、增量式性能测、手工功能性测试，示意图如下所示：
<img src="/images/k8s/pipeline.jpg" alt="" />
从图中可以看到代码的静态检测用的cpplint以及infer，后续会写一个这两个工具的对比分析；分布式功能性测试除了完成全部的自动化case回归之外，还包括代码的覆盖率统计，用的是gcov；增量式压测用的是部门内部已存在的压测工具端；手工功能测试，使用第二部分的环境进行测试即可。</p>

<p>这里重点说一下增量式压测，压测的目的是检查程序运行的稳定性以及是否存在内存泄露等问题。目前的压测环境用的线上拷贝的全量数据，而压测端用的是从线上截取的部分数据，比如从线上截取20000条。这样就存在一个问题，由于压测端的数据并不具有特异性，无法重点覆盖新增加的功能。考虑一种极端的情况，使用的这20000条数据，得到是同样的样式数据，这样就无法做到覆盖尽可能多的分支。</p>

<p>而分布式回归测试中，自动化case都是具有特异性的，就是为了验证某项功能，那么可不可以将分布式回归的case经过一些修改用于增量式压测呢？其实这个方案是可行的，在休假之前，我已经在物料库测试中进行了验证。假设在物料库服务中的新增代码中存在内存泄露，那么某个自动化case恰好覆盖这个代码分支，那么反复运行该case，内存泄露的情况从kubernetes的node节点中是可以明显查看到的。</p>

<p>由于heapster以及grafana的存在，使得监控pod以及报警变得更加简单，关于其实现机制有兴趣的同学可以去github中查看，下面的例子是当时测试过程中的截图，如下所示：
<img src="/images/k8s/material1.jpg" alt="" />
图一
<img src="/images/k8s/material2.jpg" alt="" />
图二</p>

<p>其实并不是需要运行几个小时甚至一天才能查看到内存是否泄漏，从上面的两个图可以看到即使很短的时间，或许只要几分钟我们也可以看到内存持续增长的趋势。</p>

<h4>后记</h4>
<p>这篇的内容其实是对之前我所做工作的一个梳理，虽然休假之前就想写一写，但是感到头疼，因为涉及的点太多，感觉无从下手。本篇也只是写了一个概览，有很多细节并未提及，希望本篇能给还在这条路上探索的同学一点启发。</p>

<p>很多时候我一直在想提高测试生产力的方法，其实到最后可归为两类，一类是测试技术能力的提升，比如本篇所提及的；另一类则是测试策略。之前写过一些总结类的文章，基本都会涵盖一些测试策略、测试方法。测试的策略与方法并非一成不变的，好的测试工程师应该可以根据业务的需求，定制合适的测试方案。</p>

<p>本篇就到这吧，最后欢迎关注我们的微信公众号，铸盾师~~</p>

	  ]]></description>
	</item>

	<item>
	  <title>基于Ansible && Docker的分布式系统(下)</title>
	  <link>//ansible-docker-2</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-06-28T16:28:00+00:00</pubDate>
	  <guid>//ansible-docker-2</guid>
	  <description><![CDATA[
	     <h4>前言：</h4>
<p>在上一篇中，我们主要从Poster入手，然后讲述了为何进行这样的技术选型，分别从docker和ansible两个方面来阐述了原因，本篇的重点在于分析集成于镜像中的Unicorn工程，以及playbook脚本的编写。</p>

<h4>一、Unicorn工程</h4>
<p>之所以叫Unicorn这个名称，是因为在整个系统的构建，其扮演者最重要的角色。Unicorn工程的目录结构如下图所示：</p>

<p><img src="/images/unicorn/directory.jpg" alt="" /></p>

<p>该工程主要分成5个部分，每个模块仅从其字面意思应该就可以知道大概了，这里简单的说一下</p>

<ul>
  <li>APP   ：这个应用的主目录，主要的功能就是启动worker，接收任务，执行，写入数据</li>
  <li>Conf  : 这个目录下主要放置系统相关的配置文件，其可配置的内容包括broker队列的相关信息，任务类型信息，Hook脚本配置信息，结果输出类型信息等</li>
  <li>Lib   ：该目录主要用于存放共有调用库，目前主要放置三类，一类为broker driver相关库，比如结果写入Mongodb，或者写入Mysql，或者写入文件。第二类为配置文件解析库，用于解析配置文件</li>
  <li>Script: 该目录下主要放置shell脚本和python脚本，供主程序调用</li>
  <li>Log   : 主要放置运行过程中的日志文件</li>
</ul>

<p>该工程的处理流程如下所示：</p>

<p><img src="/images/unicorn/flow.jpg" alt="" /></p>

<p>将该流程可以简单描述为 加载并解析配置文件 –&gt; 获取broker队列等相关信息 –&gt; 启动worker –&gt; 执行任务  –&gt; 结果数据处理。</p>

<p>其中需要对Script模块进行解释一下，这个模块存在的目的在于worker执行之前，以及执行完写入数据之前对数据进行一些必要的操作，包括清除脏数据，筛出符合条件的数据等。由于采用的是插件话设计，在APP主程序不需要变动的情况下，只需要变动conf文件，以及添加script脚本就可以完成上述操作。这给后续一些数据预处理操作提供了接口。</p>

<h4>二、Ansible的应用</h4>
<p>这个版本区别于上一个版本的地方主要在于Ansible的使用。用一句话概况的话，Ansible使得整个部署更可控，更加规范化。
之前的所有操作，包括物料数据同步、用例case同步、docker管理全部都是通过shell脚本来完成的，这就造成了整个系统和业务的耦合度很高。如果通过Ansible的Playbook脚本的话，将各个部分拆成独立的模块，然后不同的业务分别调用共用的模块，可以很轻松的做到一套服务一套脚本。playbook的处理流程如下所示：</p>

<p><img src="/images/unicorn/flow2.jpg" alt="" /></p>

<p>该流程概况的已经非常清楚了，就不多说了。关于如何使用Ansible来启动docker，其实主要是使用Ansible的docker-image和docker-container模块，下面是playbook脚本中关于启动容器的代码示例：</p>

<p><img src="/images/unicorn/code.jpg" alt="" /></p>

<h4>三、结语</h4>
<p>本篇主要讲述了整个工程的处理流程。原理差不多也就这样了，如果是第一次接触可以查看之前发的第二版分布式系统的相关介绍。很多事情都在发生变化，当前这个版本也有很多需要优化的地方，比如执行节点的调度、容错处理等内容，本篇都没有涉及到。</p>

<p>当分布式遇到容器编排，其实现的方式又存在很大的差别，很多已经成熟的东西我们直接拿来用就好，通过k8s来实现上述的分布式会有更大的优势，特别是运行实例动态调度变化上。技术总是日新月异的变化，我们要做的就是紧跟发展的趋势，别被甩下车。</p>

<p>最后欢迎关注我们的微信公众号”铸盾师”</p>


	  ]]></description>
	</item>

	<item>
	  <title>k8s实践日记之安装篇</title>
	  <link>//k8s-practic-install</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-05-22T17:45:00+00:00</pubDate>
	  <guid>//k8s-practic-install</guid>
	  <description><![CDATA[
	     <p>其实很早就想深入了解一下容器编排工具，但是由于各种各样的事情，被耽搁了很久。最近就抽时间认真学习了一下kubernets（K8S）,虽然对其原理还未深入的了解，但是从初步掌握的情况来看，真的是一个非常牛的框架，本系列就先从安装篇开始（Centos7下）。</p>

<h4>关于标准安装方式</h4>
<p>其实就是使用kubeadm进行安装，k8s的核心组件全部容器化
https://kubernetes.io/docs/getting-started-guides/kubeadm/
安装之前，需要注意的地方：
- 限制条件，在官方文档中放在最后了，就是Limitations部分，先进行着部分的配置
- 修改/etc/hosts文件，添加本机非lo地址和所对应的名称</p>

<h4>安装过程中存在的问题</h4>
<p>安装过程中存在很多问题，一些比较简单的，稍微查找下资料就可以解决，但是有些却要费一些功夫。除了官方提供的安装教程，网上也有很多网友自发编写的，比如下方的安装教程2.
安装教程2：
http://blog.frognew.com/2017/04/kubeadm-install-kubernetes-1.6.html
在安装教程2中介绍的已经很详细了，一般情况下按照教程安装就可以。
但有以下两点需要说明以下：</p>

<p>1、节点网络选择。
关于pod之间网络的选择，我自己选择的是weave，当然是用flannel也可以。说一下我是用weave遇到的问题。目前我使用的k8s 1.6以上版本，所以选择weave网络的时候，也应该选择对应的版本，进行安装。很多网络的教程用的还是k8s 1.5版本，所以切不可照搬。</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">1.5版本：  
kubectl apply -f https://git.io/weave-kube
1.6版本：  
kubectl apply -f https://git.io/weave-kube-1.6</code></pre></figure>

<p>选择错误的话，kubedns启动存在问题</p>

<p>2、出现的错误信息。
正常启动后，kubedns容器中错误提示如下：
reflector.go:199] k8s.io/dns/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: getsockopt: no route to host</p>

<p>通过查看网友关于此类问题的描述，初步定位于iptables的问题，通过查看iptables可以看到关于网关端口转发被禁，不知道什么原因导致，如果单条记录删除，k8s可自行恢复。初步修复无果。后来听网友推荐，使用清空iptables链，启动正常</p>

<p>解决方案地址：
https://github.com/kubernetes/kubernetes/issues/44750</p>

<h4>最官方Demo实例安装错误</h4>
<p>节点加入成功后，运行demo示例提示namespace错误</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">运行的命令如下：
kubectl create namespace sock-shop
kubectl apply -n sock-shop -f <span class="s2">"https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span>

修改为：
kubectl delete namespace sock-shop
kubectl delete -n sock-shop -f <span class="s2">"https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span>
kubectl create -f <span class="s2">"https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span></code></pre></figure>

<p>有关此问题的解决方案链接
https://github.com/microservices-demo/microservices-demo/pull/702
https://github.com/kubernetes/kubernetes.github.io/issues/3214</p>

<h4>安装Heapster 、Influxdb、Garafana</h4>
<p>参考文章：
http://www.jianshu.com/p/60069089c981
需要注意的地方：
- 重新修改端口映射，可以使用外部ip访问
- 添加external ip字段，否则无法访问</p>

<p>英文权威指南：
https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md</p>

<h4>监控部署异常排查</h4>
<p>Heapster的错误日志如下：
E0519 07:06:59.731558       1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list *api.Namespace: the server does not allow access to the requested resource (get namespaces)</p>

<p>E0519 07:07:00.050170       1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:342: Failed to list *api.Node: the server does not allow access to the requested resource (get nodes)</p>

<p>原因定位：初步判断是RABC角色机制导致的这个问题
错误的解决办法：</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">kubectl create clusterrolebinding add-on-cluster-admin <span class="se">\</span>
    --clusterrole<span class="o">=</span>cluster-admin <span class="se">\</span>
    --serviceaccount<span class="o">=</span>kube-system:default</code></pre></figure>

<p>关于此问题的英文解决方案说明：
https://github.com/kubernetes/kubeadm/issues/248</p>

<h4>结束语</h4>
<p>本篇文章是kubernets容器编排系列的第一篇文章，主要介绍了安装过程中踩的一些坑，以及一些注意事项。每个人在安装过程中可能遇到的问题都不太一样，遇到问题并不可怕，沉下心来，分析错误的原因，总会找到解决方法。</p>


	  ]]></description>
	</item>

	<item>
	  <title>基于Ansible && Docker的分布式系统(上)</title>
	  <link>//ansible-docker-1</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-03-25T17:19:00+00:00</pubDate>
	  <guid>//ansible-docker-1</guid>
	  <description><![CDATA[
	     <p>从最初的V1.0的分布式用例框架，到现在已经发展到V3.0版本，功能已经越来越完善，执行效率相比第一版也有了质的提升。遇见不同，其实就是见证全新的自己，我相信我们在这条道路上会越走越好。</p>

<h4>Poster</h4>
<p>前一段时间，部门内部举办了一场技术开放日活动，我和组内的宋大神，做了关于第三版通用分布式用例框架的Poster，一些同学对我们所做的很敢兴趣，当时因为时间有限，并且人又很多，许多地方讲述的也不太完善。趁着五一之前的少许空闲，把Poster的内容稍微梳理一下，算是我个人对这段时间的一个总结，对可能感兴趣的同学也会有一些帮助。</p>

<p>poster的内容如下图所示：
<img src="/images/pc_ads_distribution/ansible_docker_framework.jpg" alt="" /></p>

<h4>我们的痛点和目标</h4>

<p>Poster中列出了V1.0版本，和V2.0版本的痛点，这里简要的说一下。运行时间长，以及分布式节点扩展困难，以及case的粒度无法控制，主要对V1.0来说的。关于详细的一些情况，可以查看之前发的文章。</p>

<p>需要人工选择部署的主机，这个是V2.0版本的遇到的，我们设想的流程是提交任务后，框架自主的从主机列表中选择合适的主机，然后去执行任务，这在V3.0版本已经解决了，虽然方法有些简单，但是也算初步解决了问题，后续在进行完善。</p>

<p>我们的目标其实就是打造通用型、高效的用例执行框架，把其作为服务提供给开发人员和测试人员。V2.0版本虽然初步完成了这个目标，但是由于其和业务联系紧密，扩展性较差，这也是开发V3.0版本的一个原因。</p>

<h4>优势在哪？</h4>

<p>V3.0版本的分布式用例执行框架，优势主要体现在三个方面：</p>

<ul>
  <li>
    <p>更简单：
每一个服务Server，都会有自己独立的一份配置。配置之间的修改也互不影响，初次之外，配置的填写和管理，全部集成在部门内部的测试平台中，只要有相对于的修改权限，都可以修改配置，执行任务。整个过程简单明了</p>
  </li>
  <li>
    <p>更智能：
更智能则体现在主机的分布式节点部署上，通过自有的算法动态的选择主机，然后自主部署，省去了人工选择Host的麻烦，可以做到比人本身更出色的分配，调度能力。目前在分布式节点部署调度上，所试用的规则有三条：
a. 在Host主机中资源满足条件的情况下，优先在已经部署过分布式节点的host上执行部署任务
b. 如果是初次部署任务，优先在资源剩余量最大的Host上部署任务
c. 针对所部署的节点数目以及host资源，完美拆分后部署（目前还不支持）</p>
  </li>
</ul>

<p>对于规则a的规定，其实是和业务紧密联系的。由于我们容器需要加载的文件太大，所以拖取需要加载的文件需要花费大量的时间，所以在host主机资源允许的情况下，尽量在已经部署过节点的主机上部署</p>

<p>和一般分布式框架不同，本分布式用例框架采用异步抢占式的调度策略，关于该抢占策略的实现，主要是借助Celery来完成的。在之前介绍V2.0版本的时候已经做了阐述，这里不细说了。</p>

<ul>
  <li>更高效：
更高效体现在一台主机，可以同时运行多个服务实例，相比于V1.0版本，case的执行效率可以成倍提升。V2.0版本已经实现了这一方法，在V3.0版本中继承了V2.0版本中优势的地方。</li>
</ul>

<h4>Why Ansible ?</h4>

<p>Ansible的好处在poster中已经列出来了，之前我对ansible也不熟悉，但是用过之后发现真的非常方便。由于是Agentless框架，所以无需在被操控的主机上安装客户端。</p>

<p>现在我们做到一套服务，一份配置，说的更明白一点，这份配置就是ansible的 playbook脚本，而动态主机信息的获取也主要是通过Ansible的动态Inventory来实现的。配置信息填写界面如下图所示：</p>

<p>这一点和V2.0版本有显著的不同，在V2.0版本中，任务资源获取、加载、执行，完全是整合在一起。没有使用Ansible来进行任务信息，主机资源信息的收集和管理，这也导致了业务和框架的耦合度太高，迁移到其他业务的成本太大，也是做V3.0的根本原因</p>

<p>在技术分享日的当天，有很多的同学问我，为啥选择ansible而不选择SaltStack等工具呢？其实我没有使用过SaltStack，在网上看到对比说，SaltStack的执行效率要比Ansible高，但是由于我们是测试环境下部署，对于执行效率并没有太高的要求；再者部门内宋大神使用Ansible已经很长时间了，现有的很多服务部署执行，都是基于Ansible来做的，所以在执行工具的选择上就选择了Ansible。除此之外，ansible对docker支持良好，这也是一个非常重要的原因。</p>

<p>所以，是不是选择Ansible还是要具体情况具体分析。</p>

<h4>Why Docker ?</h4>

<p>选择docker的原因，在前几篇的文章中已经分析过了。poster中简单的列了几条，之前由于只在PC业务中使用docker，所以维护一份镜像就够了。现在为了适应不同的业务线，我们做到了一份服务（一个server），一份镜像，根据业务需求进行定制。将构建好的镜像推送的私有的Docker Registry中，需要部署任务的时候从中拉取，方便管理且高效</p>

<p>每一次运行任务，都会重新启用新的容器，由于容器之间不需要相互通信，所以全部都使用bridge模式，运行一次构建一次，运行结束后就删除容器，释放计算机资源，这样真正的做到了任务之间的互相不影响。</p>

<p>只要计算机资源允许，可以在同一主机上运行多个服务实例，这相当于将一台主机当几台来用，不但提高了执行效率，还充分利用了现有的计算机资源。</p>

<p>关于docker在分布式用例系统中的具体应用，如果感兴趣，可以阅读以下之前发的相关文章，关于docker就说到这吧</p>

<h4>后记</h4>

<p>V3.0版本已经解决了很多问题，但是仍有很多问题没有解决，poster中也列出了2点。这都在后续完善的范畴之内。在分享日的当天，有些同学问了我这样一个问题，如果容器之间需要相互通信该怎么办呢？其实在此之前我都没有仔细考虑过这个问题，由于我们的部署在容器中的服务，不需要在容器之间相互通信，其所依赖的其他服务全部都是Mock桩并集成在单个容器中。所以对于容器之间相互通信的解决方案，我想了几个，可能不是很完善，发出来，全当同学们之间交流了</p>

<ul>
  <li>像目前我所做的，如果其依赖的都是Mock桩，那么可以将其集成在单个容器中，这样就绕过了容器之间通信的问题</li>
  <li>如果所依赖的都是真实的服务，如果自己实现容器的调度，可以通过host模式以及端口映射来解决，但是其访问的规则也要在配置文件中写清楚防止混乱</li>
  <li>如果系统是centos7等高版本系统，可以使用一些容器编排工具，包括新版的swarm或者k8s，我自己没有使用编排工具进行过分布式用例系统设计，所以更多的建议我也无法给出了</li>
</ul>

<p>其实本篇只是梳理了一下poster的内容，对一些内容作了进一步阐释，在下篇中会讲述playbook脚本文件实现，以及集成于镜像文件中的unicorn（独角兽）工程。测试，遇见不同，遇见不同的自己。</p>

<p>最后欢迎关注我们的微信公众号”铸盾师”</p>


	  ]]></description>
	</item>

	<item>
	  <title>服务端功能测试小记</title>
	  <link>//summerize-for-server-test</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-03-25T17:19:00+00:00</pubDate>
	  <guid>//summerize-for-server-test</guid>
	  <description><![CDATA[
	     <h4>前言</h4>
<p>过年回来之后，业务的功能测试渐渐多了起来，我之前一直负责的是PC方面的测试，而现在除了负责PC的业务测试之外，还负责无线业务的测试。骤然间自己所有的时间差不多都被功能测试任务占据了，到2月份末的时候，关于测试任务的排期都排到了4月初。</p>

<p>在这功能测试的狂轰滥炸中，慢慢的对于服务端重服务的功能测试有了更多的体会，趁着周末空闲的时间整理一下，以飨各位读者。</p>

<h4>功能测试就是手工测试?</h4>
<p>早些时候一直有这样的误区，认为功能测试就是手工测试。现在在脉脉的匿名区还有好多同学在感叹，不想做功能测试了，咨询自动化测试好不好学之类的问题。</p>

<p>在某些同学的潜意识里，认为web端的功能测试就是点点点，服务端方面的功能测试也就是手动构造数据，验证逻辑，这虽然比点点点好上一些，但仍没有跳出手工测试的范畴。对于以上的观点，我个人是不认同的，我认为将功能测试完全等同于手动测试是不恰当的，同样将功能测试与自动化测试完全分开来看也是不合理的。</p>

<p>从我自己功能测试的经验来看，将功能测试转变为自动化测试的一部分是效率最高的一种方法。在阐述这个问题之前，我先大致说一下我之前测试的一般情况。当开发提交测试之后，就根据测试单中的信息，手动构造数据，然后启动服务，验证本次提测的业务逻辑，其实这也是最典型的服务端功能测试的流程。这样做的好处就是可以快速的验证本次提测的业务功能，弊端就是当需要构造的数据量太大的时候，时间的成本也会很高。</p>

<p>除此之外，使用手动构造数据进行功能测试，在多次功能回归的情况下，测试人员是崩溃的，因为开发每修改一些代码，你就要把之前的case都过一遍。PC业务线之前就是这样的做法，先进行手工功能测试，后续抽时间在填充相关的测试case。无线业务线恰恰采用了另一个方法，先抽时间将case写完，然后根据提测需要完善相关case。</p>

<p>在两条业务线实验了一段时间发现，无线业务线所采用的方法，也就是将功能测试变为自动化测试的一部分，效率要高很多。特别是由于一些需求变动，或者少量代码修改，需要验证是否影响之前所测功能的时候，效果尤为明显。这个时候我就让开发人员自己去跑一遍自动化case，而我也从重复的功能性结果验证中解放了出来。这个小主题的意义在于，是否能将现有的功能测试，整合进自动化测试中，当然不同的业务的要求也不一定一致，大家根据自己业务的特点，自行评估即可。</p>

<h4>讨论维护自动化case的必要性</h4>
<p>虽然自动化测试有很多的好处，但是维护自动化case确充满了痛苦，甚至有些时候你恨不得从此再也不用它了。让人如此仇恨的原因，每次跑case失败的太多，而且失败的case大部分是很久之前的功能，很多时候你根本就从来没有听说过这个功能，这种情况下去查看与之相关的case为何失败，我相信很多人面对这种情况，心情都不会太好。</p>

<p>通常情况下，你排查了良久，也无法判断为何某些case失败，郁闷的心情可想而知，这个时候你可能会想，如果只是回归当前提测的功能该是多么幸福的一件事。在经历了多次这种事情后，慢慢的也察觉了一些规律，以及排除某些错误case的方法。就像电视上或者生活中没有无缘无故的爱，也没有无缘无故的恨一样，在自动化case的回归的世界中，也没有无缘无故就失败的case，每一个失败的case都有其失败的原因。</p>

<p>当错误的case发生时，需要排查代码的上一个版本中该case是不是失败。一般情况下，上一个版本的case应该都是全部通过的，因为如果case不通过肯定无法上线嘛。这个时候你就对比当前代码的版本和上一个代码版本，看看究竟是修改了那些内容使得case失败了。通过代码文件静态对比，以及运行期间的gdb单步调试，我想找出失败case的原因不是难事。</p>

<p>经历过多次这样的事情后，就看的比较开了，出现失败的case也会慢慢的去分析原因，不用一出现问题就去喊开发。在这里多说一句，测试人员和开发人员应该保持相对的独立性，不要什么都依靠着开发，如果真的需要找开发来解决某些问题，你应该能大致知道问题出错可能的原因在什么地方。</p>

<h4>如何高效的写自动化case</h4>
<p>谈到写自动化case，很多同学就说，这个很简单，按照EXCEL表中或者xmind图中功能测试的用例，把所有的case都写上就好了。当然这个情况下是最理想的，把所有可能的情况都覆盖掉，但是现实情况下，你可能根本没有时间将所有的case全部写完，这个时候你就要在规定的时间内，用最少量的case完成最大的代码覆盖，拒绝重复的case，以及一些非常简单的case。</p>

<p>重复的case这个比较好理解，比如某项功能在某个测试用例中已经验证过一次了，那么就没有必要在其他的case中再验证一遍。那么什么是简单的case呢？说到简单的case，就要提及代码review了，现在很多测试不参加开发的代码review，当然各种原因都有，比如时间紧、任务重啊，或者没有这样的惯例啊等等。</p>

<p>我想说的是，如果有条件，尽量在进行完粗略的主功能验证后，开始进行代码review，代码reivew不但可以让你对所测业务理解加深，提前发现一些代码级的bug，对于编写自动化测试用例也是益处良多。比如代码中，有关于某些字段的验证，再仔细查看代码后，针对性的构造自动化case，没必要根据每一个字段分别构造case，甚至你通过查看代码某部分业务逻辑已经非常清楚了，在时间紧的情况下，可以不添加与之相关的case。</p>

<p>简单的case是建立在你对代码逻辑异常清楚的情况下，判断某业务逻辑非常简单，不值得添加用例进行覆盖的case。恩，比较绕口，但应该不难理解。</p>

<h4>框架的易用性、通用性以及高效执行</h4>
<p>当case添加完成之后，总体回归所有case时，一般为了节省时间会将case分发到多台主机上同步执行。当case的数量巨大时，这种设计思路非常有必要，以目前的无线的自动化case举例，现在差不多接近600个case，如果放在单台机子上跑的话，跑完要1个半到2个小时。</p>

<p>如果分散到三台机子上跑，可能半个小时就跑完了，case数量的不断增加，分布式执行成为必要。关于分布式构建之前已经写过文章分享过，这里就不再过多阐述了，有兴趣的同学可以自己找来看看。</p>

<p>本小节要重点谈的是框架的易用性、通用性和高效执行。易用性很好理解，就是上手非常快，只需要填写少量必须的参数，整个任务就可以跑起来了，想目前一直在使用的第一版基于jenkins的分布式系统，只需要填写本次代码的svn地址或者bin文件，然后根据功能需要在中控机上做少量修改，就可以执行了。因为上手非常容易，所以教开发自己来跑任务也不用花很多时间成本。</p>

<p>我之前设计的第二版分布式系统，解决了易用性和高效执行两个方面，但是在通用性上做的不好，所以到现在就pc业务线在用，甚至我想把无线的分布式迁移过来，也不是几行代码或者几个小时能搞定的。由于现有的框架和业务联系太紧密，导致了扩展性也不好，现在正在开发基于ansible和docker的分布式解决方案。关于这个方案我会在后续的文章中详细的谈，这里就不多说了。特别是在大部门，多业务线的情况下，一个框架的设计要兼顾几个方面，能复用就复用，不要重复的开发，浪费人力物力。</p>

<p>高效执行之前也谈到过了，就是如何在尽可能短的时间内，将程序运行完。第一版的分布式系统虽然利用分布式主机的特点，提高了执行时间，但是还是没有把现有的计算机资源利用起来。第一版的分布式系统，单主机情况下，同一时间只有一个服务实例在运行，而现有的主机资源，可以同时支持3个甚至更多的运行实例，所以第二版分布式系统和第三版就利用docker 容器规避了这个问题。对于一个通用性的工具或者框架，以上三个因素都非常重要，如果不能兼顾的情况下，根据业务需求自行取舍吧。</p>

<h4>结语</h4>
<p>啰啰嗦嗦说了这么多，主要是我自己的功能测试感悟，可能对某些方面的理解有些偏颇，大家不必较真，毕竟对于同样的问题，每个人都有每个人的看法。如果能从文章中感悟一点有益的东西，也不枉费了我周末码字的辛苦。同样欢迎有不同看法的同学留言交流……..</p>


	  ]]></description>
	</item>

	<item>
	  <title>年终总结2016,写在农历鸡年之前</title>
	  <link>//summerize-2016</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-01-27T10:19:00+00:00</pubDate>
	  <guid>//summerize-2016</guid>
	  <description><![CDATA[
	     <h4>前言：</h4>
<p>一直想抽出时间来把这一年所做的事情总结一下，但是每次开始写的时候又不知道该从何写起，毕竟一年经历了很多的事情。为了不在记述的时候产生跳跃感，我就按照时间线的顺序写吧，如果要先有一个概述的话，那2016算是稳中有升的一年~</p>

<h4>前半年：</h4>
<p>2016年的前半年，主要还是在老东家360中渡过的。那个时候我从360支付平台转到360广告平台也有一段时间了，对广告平台的理解也慢慢的变深，主要负责的是360广告平台的front端的测试。测试的内容起先负责包括广告的展现打点、计费打点、打点信息校验、广告展现兼容性测试等内容，后来也负责了一部分eapi的接口测试、kafka消息队列相关测试等内容。</p>

<p>在测试的过程中对广告平台的理解也逐渐深入，其实这相当于是一个打基础的阶段，毕竟相关业务的测试都是在这个基础之上的。测试的主要方式还是手动人工测试，除了看日志之外，很多功能的测试都是需要去手动完成，例如点睛投放平台相关测试，基本上靠人工，由于投放平台的界面变化较大，做UI自动化成本太高，任务又急，这个时候也只能靠人了。</p>

<p>当时我的leader是萍姐，反正从我转入广告平台后，一直都是萍姐带着我熟悉广告平台的很多东西，萍姐是一个很称职的leader。即使后来我要离开公司，其实对萍姐还是有很多不舍，最后走的时候萍姐对我说的话，我现在也铭记着。<em>其实在工作中遇到对自己有帮助的人都是应该感恩的，毕竟并非每个人有义务对你好，给你指导，在工作中遇到一个好的leader真的是一件很幸福的事情</em>。扯远了，其实除了leader的指导帮助之外，更多的是靠自己去实践和领悟，别人教你的东西始终是别人，想把这些东西转变成自己的，只能靠自己多实践、多想、多问！</p>

<p>我对广告平台认识加深的过程，其实是在测试凤舞项目的时候，由于这个项目牵扯到方方面面，包括广告平台的物料投放、eapi接口、消息队列、searchfront引擎前端、query检索端、index检索生成、material物料库、广告展现、front打点等等。所以当这个项目测试完毕，我对于广告平台的认识也变得更加的深入，所以实践出真知不是没有道理的。但是由于引擎端的代码并不对测试人员开放，所以引擎端的处理逻辑对测试人员是黑盒，这在一定程度上也阻碍了测试人员的进一步深入。</p>

<p>其实前半年在点睛业务团队，也做了很多自动化的东西，包括打点的网页端展示系统、基于Nodejs的轻量级日志监控系统等，原本打算基于ELK技术栈来改进现有的日志监控，由于后来离职了，只对交接的人员大致讲述了下思路。其实到新公司之后，我也了解过我做的这些工具的情况，状况很令人心碎，这些工具无人维护基本上死掉了。我觉得这个状况在各个公司都存在，很多人觉得维护旧的工具还不如直接做个新的，重复的造轮子。后来想到之前的教训，我在现在的公司才更多的强调，开发工具的稳定性以及可维护性。我的目标是即使我离职了，我开发的工具也还可以运行N年。。。</p>

<p>总之,2016前半年更多的收获是在对广告平台的认识和理解，收获的是即使是做手工测试人和人之间的差距也可以很大，收获的是测试工具的根基在于业务，对业务了解不深，为了做工具而做工具得到的只能是失败。在离开360的时候，我自己的目标就是做一名顶尖的功能测试工程师。收获有很多，当然也有很多遗憾，遗憾的是对某些业务的深入程度还不够，比如ESC统计计费，还有就是没有加强自己的开发能力，业务虽然很重要，但是开发能力也很重要，不要顾此失彼。</p>

<h4>后半年：</h4>
<p>从老东家360离开之后，便来到了现在的公司，搜狗。其实之前对搜狗并不是很了解，我影响最深的产品就是搜狗输入法，其他的真不是很清楚，后来面试之前还好好的补了下课。来搜狗也有一定的缘分吧，记得有一次，我现在的leader ZZ,在脉脉上和我随便聊了几句，感觉这个leader挺好的，后来就约了个时间过来聊聊，后来就来了搜狗，哈哈。我在搜狗这边也主要负责的是广告这块，只不过负责的内容对我来说正好是我之前不太了解的，之前在360的时候，负责的是搜索引擎之上的测试，比如广告投放平台；在这边主要是负责搜索引擎本身，主要是广告内容的检索、加载、展现、屏蔽等内容。这恰好是对我缺失的部分进行了一个弥补，毕竟在360，我也接触不到引擎级的代码。</p>

<p>刚开始接手的时候还是有难度，毕竟我之前也没接触过。刚开始最让我吃惊的是，竟然在1个月之后要进行代码串讲，相当于要在1个月的时间对引擎的代码读一遍，然后理出思路，这对刚接手的我绝对是个不小的考验。除了看之前关于引擎的笔记之外，我师傅军哥，还有佳佳，都帮了我很多，毕竟要想在较短的时间内掌握更多的东西，还是要多向前辈们请教，搜狗的同事都很nice，你只要问，同事都会乐于告诉你。</p>

<p>代码串讲之后，也就开始慢慢的接各种业务提测，在测试过程中就会发现，在熟知代码后，测试的逻辑也变的异常清晰。源码面前无秘密，这次我是深刻体会到了。测试也参与开发的代码review，确实让测试快速的成长起来。熟悉了基本的业务后，就可是熟悉各种已经存在的测试工具、系统，我接手的时候这些测试工具都维护的比较好，主要是分布式测试系统，以及压力测试系统。每次测试完功能后，将这两个功能都跑一篇，进一步查漏补缺，有好几次bug就是在查漏补缺的过程中找到的。</p>

<p>我慢慢的体会到了，分布式系统以及压测系统的重要性，后来也慢慢的接手维护的工作。到现在为止，我们每次跑的全量case数据大约在540条左右，所有的提测任务完成后，在一定的时间内就会把case添加到case集中，这形成了一个正向的循环，测试人员也慢慢的从重复的功能性回归中解放出来。现在的分布式测试系统，以及压测系统，开发也会自己去跑，我们现在所做的慢慢由之前的一次性交付向测试服务provider转变。这是一个很好的现象。</p>

<p>到了现阶段的话，除了维护测试工具的稳定性，完善case集之外，更多的是对现有的工具进行改进，提升测试效率，提高生产力。为了缩短分布式系统的执行时间，我们引进了docker，使用docker容器来作为基础单元参与调度，所有case的执行时间由原先的20多分钟缩短到5分钟。与此同时，为了管理docker容器，我们使用了swarm以及shipyard来进行基础容器编排以及拓展，并结合ansible来提升管理的效率。在压力测试中，我们正在整合现有的压测工具，使之平台化、通用化。</p>

<p>除此之外，我们还开发了新版的广告预览平台，开发测试都在使用，其实由一次性交付向持续交付转变，最多的障碍其实是在思维上。不管怎么样，我们都是在不断的改变自己，不断的利用新的技术、方法、思维来提高测试的生产力，这个方向总归是对的。</p>

<p>除了技术方面，如何让别人来分享自己的研究成果也是很重要的一方面，之前都是规定每个人都要进行分享，到后来很多同学觉得这变成了一个负担，分享的质量也有所下滑。分享结束后，如果没有参加分享的人可能只能通过看ppt来大致了解所讲述的内容，所以单纯的讲座效果并不如想象的好。现在部门内成立了技术影响力小组，老大让我负责，其实我有些诚惶诚恐，毕竟我自己的技术也很一般，只希望自己能近最大的努力把这件事情做好。大致弄了一个大纲，后续和leader去对一下，然后慢慢的把这件事做起来。</p>

<h4>结语：</h4>
<p>时间过的真的很快，一转眼2017已经来了。2016年对我来说是成长和收获的一年，希望自己和部门能够在新的一年里越来越好！2017，加油干，撸起50亿！！！</p>

	  ]]></description>
	</item>


</channel>
</rss>
