<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>airfer.github.io/</title>
   
   <link>http://airfer.github.io/</link>
   <description>There are some things in this world will never change and some things do change,everything that has a beginning has an end </description>
   <language>en-uk</language>
   <managingEditor> Wang Yukun</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>kubernetes在测试中的应用实践</title>
	  <link>//k8s-practice</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-08-29T19:28:00+00:00</pubDate>
	  <guid>//k8s-practice</guid>
	  <description><![CDATA[
	     <h4>前言</h4>
<p>在休假之前就想写一写kubernetes在测试中应用的一些事，后来因为各种原因被耽搁了下来。这次趁着休假期间的空闲，把应用方面的东西梳理一下，算做是对之前一段研究东西的总结。</p>

<p>研究kubernetes也有一段时间了，在测试方面的应用我将其归类为三个方面：分别为服务部署、环境搭建、持续集成优化 。其实每个方面都值得写一篇文章进行详细阐述，只是最近变的有些懒，就写个总的概况吧。对这个方向感兴趣的同仁欢迎拍砖，也欢迎大家留言探讨，正餐现在开始。</p>

<h4>服务部署</h4>
<p>服务部署作为k8s最主要的核心功能，在实践中被广泛应用是一件很正常的事情。其实这里讲的服务部署，主要是测试服务的部署，当然这和其他服务的部署没有本质的区别。这样做的目的是充分利用k8s集群的特性为我们服务，保证我们测试服务稳定且高效。目前我们的k8s集群上部署了两个服务，一个是jenkins，一个是镜像生成服务takara。就以takara为例来举例说明吧。</p>

<p>首先介绍一下takara，takara服务是根据用户提供的dockerfile文件生成image镜像，并将镜像推送到本地私有registry，成功后返回生成的镜像信息供后续部署使用。其工作原理如下图所示：
<img src="/images/k8s/takara.jpg" alt="" /></p>

<p>takara作为基础服务存在，如果其出现问题将影响后续所有的部署、构建过程，所以其必须稳定。后续由于多个业务线会使用该服务，为了高效快速的生成镜像减少排队情况的发生，Image 生成器会有多个。</p>

<p>从图中可以看出，HttpServer接受get/post的请求，请求的信息中包含dockerfile的地址信息、所需的依赖文件信息等。关于所依赖的物料数据来源有多种方式，例如使用rsync工具，或者直接从hadoop上获取。HttpServer的作用在于解析请求数据，并将解析后的数据写入任务调度队列，供后续worker使用。</p>

<p>由于worker生产镜像的过程是无状态的，所以可以通过kubernetes的replica set动态调整worker的数目。每一个worker位于一个pod中，由两个container组成，分别为Prepare container和Generate container。</p>

<p>前者用于docker build之前的准备工作，主要是获取dockerfile文件，以及相关的依赖文件，并将获得的文件数据进行持久化存储。当前持久化存储使用的是nfs，目前在测试环境下支持良好，速度较快。后者用于根据获取的dockerfile文件数据，生成image镜像，并将镜像重新命名，打tag后推送到私有的镜像仓库，供后续使用。</p>

<p>由于httpserver的上层是kubernetes的service，所以即使一个httpserver挂掉，也不会影响整体的运行。httpserver的数量以及worker的数量都通过replica set进行控制。可以发现，使用kubernetes进行服务部署的好处在于服务的稳定高效，无状态扩展简单。并不是所有的服务都适合在kubernetes上部署，具体情况需要具体分析。
环境搭建
在测试过程中，环境搭建是一个比较头疼的事情，各种操作系统的版本，各种依赖库，每一次搭建都要重复一遍这个痛苦的过程。那么有没有一种方法可以一劳永逸呢？其实答案就是镜像。我们将各种操作系统的版本以及服务运行所依赖的lib库生成一个镜像，然后将镜像上传到我们的私有registry，每次在使用的时候直接从registry中拖取就可以了。</p>

<p>这样看来是非常简单的事情，那么这和kubernetes有什么关系呢?我们安装docker直接pull不可以吗？其实在实际应用中会发现遇到的问题比刚开始所想的要多的多。那么考虑下面几个方面：</p>

<ul>
  <li>如果开发同学想要一个环境，但是没有空闲的机器</li>
  <li>如果你在本地环境发现某问题，如何让开发快速的定位修复</li>
  <li>如何和同组的其他同学分享你的测试环境</li>
  <li>当测试环境很多时，如何有效的管理这些环境</li>
  <li>当测试环境由于一些因素挂掉时，如何保证稳定可用</li>
</ul>

<p>通过上述的问题，我们发现要满足上面的几个方面，需要保证以下几点：</p>

<ul>
  <li>非初次环境的部署要快速，不能部署一个环境需要半天甚至更长的时间</li>
  <li>环境需要有独立的ip，可以通过ssh访问，便于开发定位问题</li>
  <li>当环境很多时，需要有一个列表或者table列出环境与ip的对应关系，方便查找</li>
  <li>当环境因某些原因挂掉时，可以自行重启恢复，重启后数据不丢失</li>
</ul>

<p>目前部门内部试用的环境部署方案如下图所示：
<img src="/images/k8s/ssh.jpg" alt="" /></p>

<p>从图中可以看出，整体的结构比较简单，其实环境部署的重点在于镜像制作。用户通过ssh登录到中控机，然后在中控机上通过ssh免密码登录所需服务。但是这里需要对以下几点进行说明：</p>

<ul>
  <li>由于集群内部中pod的ip为weave overlay网段私有ip地址，所以在公司内网中是无法直接访问。为了解决这个问题，一种方式是将镜像中的sshd服务通过service开放出来，这样通过访问service可以访问sshd服务，但是这种方式存在一些问题，比如在内网中存在external ip无法获取的情况</li>
  <li>另一种方式，将kubernetes集群中的一个node作为中控节点，通过在中控节点访问kubernetes内部的pod，目前部门内部采用的是这种方式。这种方式相比第一种方法，比较简单不需要配置service，同时也起到了一定的安全保护作用（只可从中控机访问）</li>
  <li>每个需要启动的环境镜像，都包含了中控机的ssh公钥信息，这样可以做到免密码登录</li>
  <li>至于查看环境与ip的对应关系，通过kubernetes的dashboard或者在master节点上查看都可以，不过推荐dashboard</li>
</ul>

<h4>持续集成优化</h4>
<p>持续集成一直是测试关注的重点，在持续集成的实践中，我们也碰到了一些问题。比如，我们需要选择具体在某个jenkins slave node中执行我们的程序，由于程序的执行对操作系统以及相关的依赖库都有要求，所以在运行jenkins任务之前，我们必须保证slave节点已经配置了任务要求所有的要素。</p>

<p>每新增一个slave节点，上述的环境配置都需要重新配置，很耗费时间。同时如果我们的slave节点挂掉了，我们需要去重新启动这个节点；如果这个节点已经下线，那么就需要重新申请一个节点，或者将该任务的执行节点重新迁移到新的节点。</p>

<p>当任务不多，并且节点变动不是很频繁时，这个需求并不是很强烈。但是当支持的业务线众多，各服务依赖的操作系统以及相关库差异性很大时，基于容器的持续集成就变得优势巨大。每个业务线都可以根据自己的需求定制自己的镜像，持续部署时依赖自己的镜像就可以了。</p>

<p>举个例子，假设有5台slave机子都安装了某依赖库，后来由于服务升级，该依赖库也需要进行升级。如果按照传统的方法，需要分别到这5台机子上进行升级，而如果使用镜像，只需要重新生成镜像即可。</p>

<p>使用kubernetes集群进行持续集成最大的优点就是解决了环境依赖问题，最终这个任务究竟在哪个node，哪个pod中运行，根本无需担心。目前基于kubernetes集群的持续集成有两个方式：
一种是直接在master节点上按照pipeline的stage执行程序，程序直接在master节点上执行，和服务部署相同，程序运行结束，pod就终止被删除。
另一种方式，是利用jenkins的kubernetes插件，关于其使用说明github中的文档已经写的很清楚了，就不再赘述了。</p>

<p>关于pipeline的使用并不是本篇讨论的重点，后续@宋大神会专门写一篇文章介绍。</p>

<p>这里以目前的竞价服务的pipeline进行简要的说明。对于竞价服务，和其他服务一样，我们需要一个完整的持续集成体系。在这个持续集成的pipeline中，包含代码静态检测、分布式功能性测试、增量式性能测、手工功能性测试，示意图如下所示：
<img src="/images/k8s/pipeline.jpg" alt="" />
从图中可以看到代码的静态检测用的cpplint以及infer，后续会写一个这两个工具的对比分析；分布式功能性测试除了完成全部的自动化case回归之外，还包括代码的覆盖率统计，用的是gcov；增量式压测用的是部门内部已存在的压测工具端；手工功能测试，使用第二部分的环境进行测试即可。</p>

<p>这里重点说一下增量式压测，压测的目的是检查程序运行的稳定性以及是否存在内存泄露等问题。目前的压测环境用的线上拷贝的全量数据，而压测端用的是从线上截取的部分数据，比如从线上截取20000条。这样就存在一个问题，由于压测端的数据并不具有特异性，无法重点覆盖新增加的功能。考虑一种极端的情况，使用的这20000条数据，得到是同样的样式数据，这样就无法做到覆盖尽可能多的分支。</p>

<p>而分布式回归测试中，自动化case都是具有特异性的，就是为了验证某项功能，那么可不可以将分布式回归的case经过一些修改用于增量式压测呢？其实这个方案是可行的，在休假之前，我已经在物料库测试中进行了验证。假设在物料库服务中的新增代码中存在内存泄露，那么某个自动化case恰好覆盖这个代码分支，那么反复运行该case，内存泄露的情况从kubernetes的node节点中是可以明显查看到的。</p>

<p>由于heapster以及grafana的存在，使得监控pod以及报警变得更加简单，关于其实现机制有兴趣的同学可以去github中查看，下面的例子是当时测试过程中的截图，如下所示：
<img src="/images/k8s/material1.jpg" alt="" />
图一
<img src="/images/k8s/material2.jpg" alt="" />
图二</p>

<p>其实并不是需要运行几个小时甚至一天才能查看到内存是否泄漏，从上面的两个图可以看到即使很短的时间，或许只要几分钟我们也可以看到内存持续增长的趋势。</p>

<h4>后记</h4>
<p>这篇的内容其实是对之前我所做工作的一个梳理，虽然休假之前就想写一写，但是感到头疼，因为涉及的点太多，感觉无从下手。本篇也只是写了一个概览，有很多细节并未提及，希望本篇能给还在这条路上探索的同学一点启发。</p>

<p>很多时候我一直在想提高测试生产力的方法，其实到最后可归为两类，一类是测试技术能力的提升，比如本篇所提及的；另一类则是测试策略。之前写过一些总结类的文章，基本都会涵盖一些测试策略、测试方法。测试的策略与方法并非一成不变的，好的测试工程师应该可以根据业务的需求，定制合适的测试方案。</p>

<p>本篇就到这吧，最后欢迎关注我们的微信公众号，铸盾师~~</p>

	  ]]></description>
	</item>

	<item>
	  <title>基于Ansible && Docker的分布式系统(下)</title>
	  <link>//ansible-docker-2</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-06-28T16:28:00+00:00</pubDate>
	  <guid>//ansible-docker-2</guid>
	  <description><![CDATA[
	     <h4>前言：</h4>
<p>在上一篇中，我们主要从Poster入手，然后讲述了为何进行这样的技术选型，分别从docker和ansible两个方面来阐述了原因，本篇的重点在于分析集成于镜像中的Unicorn工程，以及playbook脚本的编写。</p>

<h4>一、Unicorn工程</h4>
<p>之所以叫Unicorn这个名称，是因为在整个系统的构建，其扮演者最重要的角色。Unicorn工程的目录结构如下图所示：</p>

<p><img src="/images/unicorn/directory.jpg" alt="" /></p>

<p>该工程主要分成5个部分，每个模块仅从其字面意思应该就可以知道大概了，这里简单的说一下</p>

<ul>
  <li>APP   ：这个应用的主目录，主要的功能就是启动worker，接收任务，执行，写入数据</li>
  <li>Conf  : 这个目录下主要放置系统相关的配置文件，其可配置的内容包括broker队列的相关信息，任务类型信息，Hook脚本配置信息，结果输出类型信息等</li>
  <li>Lib   ：该目录主要用于存放共有调用库，目前主要放置三类，一类为broker driver相关库，比如结果写入Mongodb，或者写入Mysql，或者写入文件。第二类为配置文件解析库，用于解析配置文件</li>
  <li>Script: 该目录下主要放置shell脚本和python脚本，供主程序调用</li>
  <li>Log   : 主要放置运行过程中的日志文件</li>
</ul>

<p>该工程的处理流程如下所示：</p>

<p><img src="/images/unicorn/flow.jpg" alt="" /></p>

<p>将该流程可以简单描述为 加载并解析配置文件 –&gt; 获取broker队列等相关信息 –&gt; 启动worker –&gt; 执行任务  –&gt; 结果数据处理。</p>

<p>其中需要对Script模块进行解释一下，这个模块存在的目的在于worker执行之前，以及执行完写入数据之前对数据进行一些必要的操作，包括清除脏数据，筛出符合条件的数据等。由于采用的是插件话设计，在APP主程序不需要变动的情况下，只需要变动conf文件，以及添加script脚本就可以完成上述操作。这给后续一些数据预处理操作提供了接口。</p>

<h4>二、Ansible的应用</h4>
<p>这个版本区别于上一个版本的地方主要在于Ansible的使用。用一句话概况的话，Ansible使得整个部署更可控，更加规范化。
之前的所有操作，包括物料数据同步、用例case同步、docker管理全部都是通过shell脚本来完成的，这就造成了整个系统和业务的耦合度很高。如果通过Ansible的Playbook脚本的话，将各个部分拆成独立的模块，然后不同的业务分别调用共用的模块，可以很轻松的做到一套服务一套脚本。playbook的处理流程如下所示：</p>

<p><img src="/images/unicorn/flow2.jpg" alt="" /></p>

<p>该流程概况的已经非常清楚了，就不多说了。关于如何使用Ansible来启动docker，其实主要是使用Ansible的docker-image和docker-container模块，下面是playbook脚本中关于启动容器的代码示例：</p>

<p><img src="/images/unicorn/code.jpg" alt="" /></p>

<h4>三、结语</h4>
<p>本篇主要讲述了整个工程的处理流程。原理差不多也就这样了，如果是第一次接触可以查看之前发的第二版分布式系统的相关介绍。很多事情都在发生变化，当前这个版本也有很多需要优化的地方，比如执行节点的调度、容错处理等内容，本篇都没有涉及到。</p>

<p>当分布式遇到容器编排，其实现的方式又存在很大的差别，很多已经成熟的东西我们直接拿来用就好，通过k8s来实现上述的分布式会有更大的优势，特别是运行实例动态调度变化上。技术总是日新月异的变化，我们要做的就是紧跟发展的趋势，别被甩下车。</p>

<p>最后欢迎关注我们的微信公众号”铸盾师”</p>


	  ]]></description>
	</item>

	<item>
	  <title>k8s实践日记之安装篇</title>
	  <link>//k8s-practic-install</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-05-22T17:45:00+00:00</pubDate>
	  <guid>//k8s-practic-install</guid>
	  <description><![CDATA[
	     <p>其实很早就想深入了解一下容器编排工具，但是由于各种各样的事情，被耽搁了很久。最近就抽时间认真学习了一下kubernets（K8S）,虽然对其原理还未深入的了解，但是从初步掌握的情况来看，真的是一个非常牛的框架，本系列就先从安装篇开始（Centos7下）。</p>

<h4>关于标准安装方式</h4>
<p>其实就是使用kubeadm进行安装，k8s的核心组件全部容器化
https://kubernetes.io/docs/getting-started-guides/kubeadm/
安装之前，需要注意的地方：
- 限制条件，在官方文档中放在最后了，就是Limitations部分，先进行着部分的配置
- 修改/etc/hosts文件，添加本机非lo地址和所对应的名称</p>

<h4>安装过程中存在的问题</h4>
<p>安装过程中存在很多问题，一些比较简单的，稍微查找下资料就可以解决，但是有些却要费一些功夫。除了官方提供的安装教程，网上也有很多网友自发编写的，比如下方的安装教程2.
安装教程2：
http://blog.frognew.com/2017/04/kubeadm-install-kubernetes-1.6.html
在安装教程2中介绍的已经很详细了，一般情况下按照教程安装就可以。
但有以下两点需要说明以下：</p>

<p>1、节点网络选择。
关于pod之间网络的选择，我自己选择的是weave，当然是用flannel也可以。说一下我是用weave遇到的问题。目前我使用的k8s 1.6以上版本，所以选择weave网络的时候，也应该选择对应的版本，进行安装。很多网络的教程用的还是k8s 1.5版本，所以切不可照搬。</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">1.5版本：  
kubectl apply -f https://git.io/weave-kube
1.6版本：  
kubectl apply -f https://git.io/weave-kube-1.6</code></pre></figure>

<p>选择错误的话，kubedns启动存在问题</p>

<p>2、出现的错误信息。
正常启动后，kubedns容器中错误提示如下：
reflector.go:199] k8s.io/dns/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: getsockopt: no route to host</p>

<p>通过查看网友关于此类问题的描述，初步定位于iptables的问题，通过查看iptables可以看到关于网关端口转发被禁，不知道什么原因导致，如果单条记录删除，k8s可自行恢复。初步修复无果。后来听网友推荐，使用清空iptables链，启动正常</p>

<p>解决方案地址：
https://github.com/kubernetes/kubernetes/issues/44750</p>

<h4>最官方Demo实例安装错误</h4>
<p>节点加入成功后，运行demo示例提示namespace错误</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">运行的命令如下：
kubectl create namespace sock-shop
kubectl apply -n sock-shop -f <span class="s2">"https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span>

修改为：
kubectl delete namespace sock-shop
kubectl delete -n sock-shop -f <span class="s2">"https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span>
kubectl create -f <span class="s2">"https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span></code></pre></figure>

<p>有关此问题的解决方案链接
https://github.com/microservices-demo/microservices-demo/pull/702
https://github.com/kubernetes/kubernetes.github.io/issues/3214</p>

<h4>安装Heapster 、Influxdb、Garafana</h4>
<p>参考文章：
http://www.jianshu.com/p/60069089c981
需要注意的地方：
- 重新修改端口映射，可以使用外部ip访问
- 添加external ip字段，否则无法访问</p>

<p>英文权威指南：
https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md</p>

<h4>监控部署异常排查</h4>
<p>Heapster的错误日志如下：
E0519 07:06:59.731558       1 reflector.go:203] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:84: Failed to list *api.Namespace: the server does not allow access to the requested resource (get namespaces)</p>

<p>E0519 07:07:00.050170       1 reflector.go:203] k8s.io/heapster/metrics/sources/kubelet/kubelet.go:342: Failed to list *api.Node: the server does not allow access to the requested resource (get nodes)</p>

<p>原因定位：初步判断是RABC角色机制导致的这个问题
错误的解决办法：</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">kubectl create clusterrolebinding add-on-cluster-admin <span class="se">\</span>
    --clusterrole<span class="o">=</span>cluster-admin <span class="se">\</span>
    --serviceaccount<span class="o">=</span>kube-system:default</code></pre></figure>

<p>关于此问题的英文解决方案说明：
https://github.com/kubernetes/kubeadm/issues/248</p>

<h4>结束语</h4>
<p>本篇文章是kubernets容器编排系列的第一篇文章，主要介绍了安装过程中踩的一些坑，以及一些注意事项。每个人在安装过程中可能遇到的问题都不太一样，遇到问题并不可怕，沉下心来，分析错误的原因，总会找到解决方法。</p>


	  ]]></description>
	</item>

	<item>
	  <title>基于Ansible && Docker的分布式系统(上)</title>
	  <link>//ansible-docker-1</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-03-25T17:19:00+00:00</pubDate>
	  <guid>//ansible-docker-1</guid>
	  <description><![CDATA[
	     <p>从最初的V1.0的分布式用例框架，到现在已经发展到V3.0版本，功能已经越来越完善，执行效率相比第一版也有了质的提升。遇见不同，其实就是见证全新的自己，我相信我们在这条道路上会越走越好。</p>

<h4>Poster</h4>
<p>前一段时间，部门内部举办了一场技术开放日活动，我和组内的宋大神，做了关于第三版通用分布式用例框架的Poster，一些同学对我们所做的很敢兴趣，当时因为时间有限，并且人又很多，许多地方讲述的也不太完善。趁着五一之前的少许空闲，把Poster的内容稍微梳理一下，算是我个人对这段时间的一个总结，对可能感兴趣的同学也会有一些帮助。</p>

<p>poster的内容如下图所示：
<img src="/images/pc_ads_distribution/ansible_docker_framework.jpg" alt="" /></p>

<h4>我们的痛点和目标</h4>

<p>Poster中列出了V1.0版本，和V2.0版本的痛点，这里简要的说一下。运行时间长，以及分布式节点扩展困难，以及case的粒度无法控制，主要对V1.0来说的。关于详细的一些情况，可以查看之前发的文章。</p>

<p>需要人工选择部署的主机，这个是V2.0版本的遇到的，我们设想的流程是提交任务后，框架自主的从主机列表中选择合适的主机，然后去执行任务，这在V3.0版本已经解决了，虽然方法有些简单，但是也算初步解决了问题，后续在进行完善。</p>

<p>我们的目标其实就是打造通用型、高效的用例执行框架，把其作为服务提供给开发人员和测试人员。V2.0版本虽然初步完成了这个目标，但是由于其和业务联系紧密，扩展性较差，这也是开发V3.0版本的一个原因。</p>

<h4>优势在哪？</h4>

<p>V3.0版本的分布式用例执行框架，优势主要体现在三个方面：</p>

<ul>
  <li>
    <p>更简单：
每一个服务Server，都会有自己独立的一份配置。配置之间的修改也互不影响，初次之外，配置的填写和管理，全部集成在部门内部的测试平台中，只要有相对于的修改权限，都可以修改配置，执行任务。整个过程简单明了</p>
  </li>
  <li>
    <p>更智能：
更智能则体现在主机的分布式节点部署上，通过自有的算法动态的选择主机，然后自主部署，省去了人工选择Host的麻烦，可以做到比人本身更出色的分配，调度能力。目前在分布式节点部署调度上，所试用的规则有三条：
a. 在Host主机中资源满足条件的情况下，优先在已经部署过分布式节点的host上执行部署任务
b. 如果是初次部署任务，优先在资源剩余量最大的Host上部署任务
c. 针对所部署的节点数目以及host资源，完美拆分后部署（目前还不支持）</p>
  </li>
</ul>

<p>对于规则a的规定，其实是和业务紧密联系的。由于我们容器需要加载的文件太大，所以拖取需要加载的文件需要花费大量的时间，所以在host主机资源允许的情况下，尽量在已经部署过节点的主机上部署</p>

<p>和一般分布式框架不同，本分布式用例框架采用异步抢占式的调度策略，关于该抢占策略的实现，主要是借助Celery来完成的。在之前介绍V2.0版本的时候已经做了阐述，这里不细说了。</p>

<ul>
  <li>更高效：
更高效体现在一台主机，可以同时运行多个服务实例，相比于V1.0版本，case的执行效率可以成倍提升。V2.0版本已经实现了这一方法，在V3.0版本中继承了V2.0版本中优势的地方。</li>
</ul>

<h4>Why Ansible ?</h4>

<p>Ansible的好处在poster中已经列出来了，之前我对ansible也不熟悉，但是用过之后发现真的非常方便。由于是Agentless框架，所以无需在被操控的主机上安装客户端。</p>

<p>现在我们做到一套服务，一份配置，说的更明白一点，这份配置就是ansible的 playbook脚本，而动态主机信息的获取也主要是通过Ansible的动态Inventory来实现的。配置信息填写界面如下图所示：</p>

<p>这一点和V2.0版本有显著的不同，在V2.0版本中，任务资源获取、加载、执行，完全是整合在一起。没有使用Ansible来进行任务信息，主机资源信息的收集和管理，这也导致了业务和框架的耦合度太高，迁移到其他业务的成本太大，也是做V3.0的根本原因</p>

<p>在技术分享日的当天，有很多的同学问我，为啥选择ansible而不选择SaltStack等工具呢？其实我没有使用过SaltStack，在网上看到对比说，SaltStack的执行效率要比Ansible高，但是由于我们是测试环境下部署，对于执行效率并没有太高的要求；再者部门内宋大神使用Ansible已经很长时间了，现有的很多服务部署执行，都是基于Ansible来做的，所以在执行工具的选择上就选择了Ansible。除此之外，ansible对docker支持良好，这也是一个非常重要的原因。</p>

<p>所以，是不是选择Ansible还是要具体情况具体分析。</p>

<h4>Why Docker ?</h4>

<p>选择docker的原因，在前几篇的文章中已经分析过了。poster中简单的列了几条，之前由于只在PC业务中使用docker，所以维护一份镜像就够了。现在为了适应不同的业务线，我们做到了一份服务（一个server），一份镜像，根据业务需求进行定制。将构建好的镜像推送的私有的Docker Registry中，需要部署任务的时候从中拉取，方便管理且高效</p>

<p>每一次运行任务，都会重新启用新的容器，由于容器之间不需要相互通信，所以全部都使用bridge模式，运行一次构建一次，运行结束后就删除容器，释放计算机资源，这样真正的做到了任务之间的互相不影响。</p>

<p>只要计算机资源允许，可以在同一主机上运行多个服务实例，这相当于将一台主机当几台来用，不但提高了执行效率，还充分利用了现有的计算机资源。</p>

<p>关于docker在分布式用例系统中的具体应用，如果感兴趣，可以阅读以下之前发的相关文章，关于docker就说到这吧</p>

<h4>后记</h4>

<p>V3.0版本已经解决了很多问题，但是仍有很多问题没有解决，poster中也列出了2点。这都在后续完善的范畴之内。在分享日的当天，有些同学问了我这样一个问题，如果容器之间需要相互通信该怎么办呢？其实在此之前我都没有仔细考虑过这个问题，由于我们的部署在容器中的服务，不需要在容器之间相互通信，其所依赖的其他服务全部都是Mock桩并集成在单个容器中。所以对于容器之间相互通信的解决方案，我想了几个，可能不是很完善，发出来，全当同学们之间交流了</p>

<ul>
  <li>像目前我所做的，如果其依赖的都是Mock桩，那么可以将其集成在单个容器中，这样就绕过了容器之间通信的问题</li>
  <li>如果所依赖的都是真实的服务，如果自己实现容器的调度，可以通过host模式以及端口映射来解决，但是其访问的规则也要在配置文件中写清楚防止混乱</li>
  <li>如果系统是centos7等高版本系统，可以使用一些容器编排工具，包括新版的swarm或者k8s，我自己没有使用编排工具进行过分布式用例系统设计，所以更多的建议我也无法给出了</li>
</ul>

<p>其实本篇只是梳理了一下poster的内容，对一些内容作了进一步阐释，在下篇中会讲述playbook脚本文件实现，以及集成于镜像文件中的unicorn（独角兽）工程。测试，遇见不同，遇见不同的自己。</p>

<p>最后欢迎关注我们的微信公众号”铸盾师”</p>


	  ]]></description>
	</item>

	<item>
	  <title>基于 Docker 的分布式测试系统构建 (二)</title>
	  <link>//pc-ads-distribution-docker</link>
	  <author>Wang Yukun</author>
	  <pubDate>2016-11-06T15:18:00+00:00</pubDate>
	  <guid>//pc-ads-distribution-docker</guid>
	  <description><![CDATA[
	     <p><strong>前言：</strong>
在基于<a href="https://testerhome.com/topics/6184">Docker的分布式测试系统构建（一）</a>中主要阐述了两个方面的内容，分别为开发此分布式测试系统的缘由以及docker基础镜像的构建和踩过的坑。在本篇中主要有4个部分的内容，分别为分布式测试系统的架构、技术实现细节简述、docker Node节点的部署，以及前端实现。</p>

<blockquote>
  <p>—-  搜狗ADTQ测试组出品</p>
</blockquote>

<hr />

<h5>一、分布式测试系统的架构</h5>
<hr />
<p>整体的测试架构主要Docker Nodes节点，Mongodb Broker，Mongodb DB, Front Web，这4个部分构成，其实现的架构简图如下所示：
<img src="/images/pc_ads_distribution/base-framework.jpg" alt="" />
现在就每个部分简要叙述：</p>

<p><em>1. Docker Node节点简述</em>
Docker Node节点主要有Celery 异步框架，Celery Worker任务，Nosetests 测试框架，Mock服务构成。
其分布式实现主要有Celery来完成，通过编写Celery worker 任务来实现具体的测试逻辑。由于业务逻辑本身的需求情况，分层级的调用关系成为实现的有效途径。Celery Worker从Mongodb Broker接收需要完成的任务信息，然后调用Node节点本身的Nosetests框架和Shell脚本，最后调用Mock Service辅助完成测试任务。Celery Worker完成待定的测试任务后，将测试结果写入MongoBD 数据库中以备前端调用。</p>

<p><em>2. Docker Swarm</em>
由于本系统的docker node节点并不多，考虑实现成本以及后续管理难易程度，本系统使用Swarm来管理docker node节点。如果docker node节点众多，可以考虑k8s。关于docker swarm在第二部分会进行说明，这里不在叙述。</p>

<p><em>3. Mogondb</em>
在本系统中，MongoDB主要有两个方面的用途。用途一，作为Celery 的broker，接收前端发送过来的任务请求信息，当broker中有数据时，Celery Worker从broker中获取数据，完成后续任务执行过程；用途二，作为DataBase，Celery worker完成任务后，将result.json数据写入到数据库中</p>

<p><em>4. 前端实现</em>
由于部门内当前的web系统，使用的是Flask Web框架，所以前端主要由Flask + Jquery + Bootstrap实现
整个架构实现还算比较清晰，这对后期的维护也带来了方便。</p>

<h5>二、技术实现有关细节简述</h5>
<hr />

<p><em>1. 关于使用Celery作为异步框架</em>
由于测试用例本身都是基于python来开发的，并且web系统为Flask，所以有充分的理由选择Celery,关于异步框架的选择可以参考<a href="https://testerhome.com/topics/5732">基于 Docker 集群的分布式测试系统 DDT (DockerDistributedTest)</a>文章中对异步框架的对比。</p>

<p><em>2. 关于Nosetests的插件化</em>
默认安装的nosetests是并未安装json插件的，但是提供xml格式。为了便于后续的结果处理，需要事先安装json插件。这里还有一个小插曲，由于之前从来都是使用默认的插件，都不太清楚原来nosetests也可以私自开发插件并安装，走了不少弯路。当然json的插件不需要自己再重复造轮子，可以直接下载安装。</p>

<p>由于nosetests提供了xml格式报告输出，所以第一时间选择的是xml作为最后的结果报文格式。用到–with-xunit参数，同时需要安装nosexunit插件，但是安装的过程中，需要coverage特定版本的支持，为2.85版本。但是即使安装了2.85版本的支持包，<a href="https://pypi.python.org/pypi/coverage/2.85">https://pypi.python.org/pypi/coverage/2.85</a>也同样会报错。</p>

<p>安装后，使用–with-xunit来运行，会提示如下信息：<em>NameError: global name ‘pylint’ is not defined</em>。但是pylint已经被正确安装，出现这个错误很是匪夷所思。后来以为是nosexunit的版本的问题，现在安装的是最新版本0.3.3，后来换成0.3.2和0.3.1都会出现错误，暂时还没有发现原因。所以最后还是使用json作为结果输出格式。</p>

<p><em>3. 关于测试数据以及环境准备</em>
数据更新以及运行相关环境的准备，总体来说可以有两种大的途径，可总结为远程拉取，和本地挂载。</p>

<ul>
  <li>远程拉取</li>
</ul>

<p>远程拉取可以有多种方法，第一种为通过rsync方法，将目的机中的数据远程拉取到docker node中，但是由于运行环境的数据量太大，所以当初认为并不可行。经过统计，在内网中完全拉取大小在5.5G左右的文件夹，需要6分钟左右，这个时间太长了。所以当初这个方案是被废弃了。</p>

<p>第二种方法为使用svn或者git，svn的话对于一些特别大的文件，会提示上传受到限制，这种情况下可以使用svn ignore对某些大的文件进行排除，后来发现由于文件夹太大的原因导致svn报错，使用svn ignore属性同样不能解决问题，现在svn报错也还无法查明原因。</p>

<ul>
  <li>本地挂载</li>
</ul>

<p>通过将所需要的文件传输到docker node宿主机中，然后在运行docker node的时候通过docker -v 本地挂载的形式，可以比较方便的解决环境问题。但是这样会使得部署一个node节点也复杂了一步，必须先同步环境相关数据到docker宿主机，这样又绕回了问题本身。所以本地挂载在本分布式测试系统中并不合适。</p>

<p>所以最后还是要考虑第一个方案。在仔细研究了rsync服务后，发现之前对rsync的研究并不深入，并不清楚rsync的差异性同步模式，通过这篇文章进行了详细的了解，<a href="https://segmentfault.com/a/1190000002427568">https://segmentfault.com/a/1190000002427568</a>，最后决定使用rsync的方式来进行文件同步。
举例如下：</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">rsync -auvrtzopgP --progress --delete  --exclude <span class="s2">"core.*"</span>   --exclude <span class="s2">"your/log"</span> 192.168.56.73::root/the/des/directory/  ./ </code></pre></figure>

<p>通过设置–exclude 参数可以将不需要同步的文件排除，比如日志文件，这在实践中很有用</p>

<p><em>4. 关于分布式系统的执行粒度</em>
在第一篇文章中我有提，现在的分布式系统执行的最小粒度是文件，也就是说nosetests 在运行测试程序时，最小是全部执行某一个文件中所有的case。比如A.py文件中有10个cases，而B.py文件有50cases，那么运行过程是node1运行A.py中的所有case，node2运行B.py中的所有cases。由于A.py中的cases比较少，所以node1运行完成后，就闲置了。而总的运行时间有node2来决定，这种情况下系统的资源被浪费了。</p>

<p>当以casesID作为执行的基本单元时，这种情况就不复存在了，假如运行一个case为1分钟，那么原先需要运行50min才能运行完所有的case。而在现有的执行粒度下，只需要30min就可以运行完所有的case。关于python文件中casesID的收集，可以通过nosetests –collect-only命令来进行收集。</p>

<p><em>5. 关于Celery worker的命令</em>
由于每一个cases的运行都需要Mock Service的支持，其主程序在内存中只允许运行一个实例，所以每一个docker node节点每次只可以运行一个case，否则便会相互影响。在解决这个问题的时候走了不少弯路，后来在仔细研究了celery worker的命令后，发现可以通过celery worker的运行时参数就可以控制，当时便有一个柳暗花明又一村的感觉。
举例如下：</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">celery -A pc_ads_distribute_worker worker -c 1 --maxtasksperchild<span class="o">=</span>1 -l INFO</code></pre></figure>

<p>其中的-c参数表示worker并发为1，–maxtasksperchild表示每一个worker最多有几个孩子，同样设置为1，这样就可以满足具体业务测试要求了</p>

<h5>三、docker node节点部署</h5>
<hr />

<p>在已经有了docker 镜像后，需要搭建私有的docker registry来方面docker宿主机更新docker镜像，关于docker 私有registry的搭建可以参考如下的网址，这里不再累述了：</p>

<ul>
  <li>
    <p><a href="https://github.com/docker/distribution/blob/master/docs/index.md">https://github.com/docker/distribution/blob/master/docs/index.md</a></p>
  </li>
  <li>
    <p><a href="https://github.com/docker/docker.github.io/blob/master/registry/index.md">https://github.com/docker/docker.github.io/blob/master/registry/index.md</a></p>
  </li>
  <li>
    <p><a href="http://www.open-open.com/lib/view/open1456539405281.html">http://www.open-open.com/lib/view/open1456539405281.html</a></p>
  </li>
</ul>

<p>关于具体的使用方法示例如下：</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell"><span class="c1">#Images查询地址：</span>
<span class="nb">curl  </span>http://192.168.56.73:5000/v2/_catalog
<span class="c1">#Tags查询：</span>
<span class="nb">curl  </span>http://192.168.56.73:5000/v2/pc/centos6.6_base/tags/list
<span class="c1">#具体使用方法：</span>
docker tag centos6.6:program_auto_v3.6 192.168.56.73:5000/pc/program_auto_v3.6
docker push 192.168.56.73:5000/pc/program_auto_v3.6
docker pull 192.168.56.73:5000/pc/program_auto_v3.6</code></pre></figure>

<p>由于我测试系统中，并未升级到引擎的1.2.1版本，所以需要下载额外的swarm镜像来完成，关于如何通过swarm来管理docker node镜像由于比较简单，就不多写了，有兴趣的可以参考<a href="http://dockone.io/article/227">http://dockone.io/article/227</a>来配置。</p>

<h5>四、前端部分</h5>
<hr />

<p>前端的展示主要有三个部分组成，分别为生成分布式任务、测试任务预览、测试任务统计与查询</p>

<p><em>1. 生成分布式任务及测试任务预览</em>
<img src="/images/pc_ads_distribution/distribute_auto_pic1.jpg" alt="" /></p>

<p>左边栏包含两个主要部分，分别为填写svn的地址以及上传文件。
svn地址为必填，因为分布式测试任务必须要明确是对svn的哪个版本进行的分布式测试；上传文件为可选，如果不上传文件，则运行所有测试用例，如果上传文件，则只运行上传文件中写明的测试用例文件以及确定的caseid，这个功能在只需要运行测试用例集中的某一个子集时比较有用，右边为生成的任务预览模块，每生成一次任务则新增一条记录。</p>

<p><em>2. 任务执行统计</em>
<img src="/images/pc_ads_distribution/distribute_auto_pic2.png" alt="" />
这个图比较清楚就不再说明了</p>

<p><em>3. 具体任务查询</em>
<img src="/images/pc_ads_distribution/distribute_auto_pic3.jpg" alt="" /></p>

<p>可查询某一次具体的执行情况，必须根据运行的成功还是失败，或者根据具体的文件名来查询都可以</p>

<h5>五、结语</h5>
<hr />

<p>这两篇算是对之前所做工作的一个总结吧。一天连写两篇，有种写吐了的感觉，好了，就这样吧，希望对各位童鞋有帮助……</p>


	  ]]></description>
	</item>

	<item>
	  <title>基于 Docker 的分布式测试系统构建 (一)</title>
	  <link>//pc_ads-distribution-prepare-docker</link>
	  <author>Wang Yukun</author>
	  <pubDate>2016-10-25T19:18:00+00:00</pubDate>
	  <guid>//pc_ads-distribution-prepare-docker</guid>
	  <description><![CDATA[
	     <p><strong>前言</strong></p>

<p>关于如何运用docker来解决现实问题并非是一个新鲜的问题，关于docker在测试方面的应用也有很多的文章，本篇是在前人的基础上研究了如何结合业务逻辑构建基于docker的分布式测试系统。当然初次研究肯定有很多不完善的地方，希望抛砖引玉，如果在阅读过程中有什么问题，欢迎大家留言交流。废话不多说了，本系列共2篇6个部分，涵盖了从项目开始到结束的所有基本过程。本篇介绍前2个部分，分别为构建测试系统的需求以及搭建docker镜像，希望本篇文章对你有所帮助。</p>

<blockquote>
  <p>—-  搜狗ADTQ测试组出品</p>
</blockquote>

<hr />

<h5> 一、为何要做这件事 </h5>
<p>现在部门内每次有新的需求上线，都会对之前所有的cases进行回归。cases的数量比较大，每次回归case需要花费很长的时间，部门内现在已有一个分布式的回归工具，是基于Jenkins的api和Shell来做的，虽然实现了基本的分布式的功能，但是也有一些弊端在后续的实践中慢慢显现，主要有以下几个方面：</p>

<ol>
  <li>
    <p><em>case回归的时间很长</em>。case回归的时间长主要有两个方面的原因造成：第一个原因是分布式节点比较少，这使得分布式的优势并没有完全凸显出来；第二个原因是轮询检查子节点的完成情况，并设置超时的时间，超时时间过短则不能完成收集子节点结果的任务，而超时时间过长则进一步延长了case回归的时间</p>
  </li>
  <li>
    <p><em>分布式子节点新增困难</em>。当case量上升，需要新增子节点的时候，发现这并非是一件很容易的事情。由于子节点是jenkins的node节点，所以每次新增节点都相当于部署一个jenkins节点，并需要将任务代码rsync到新的节点中。这对于一个新手来说是很不方便的，不利于后期维护。</p>
  </li>
  <li>
    <p><em>结果展示不够直观</em>。现有的结果展示只有邮件的形式，当运行完成之后通过邮件来通知分布式程序运行的结果，如果你想查看某一个文件中case的执行情况，比如该文件中有多少case成功，或者有多少case失败，抱歉，这并不提供这个功能。你收到的邮件只是一堆失败case的集合。</p>
  </li>
</ol>

<p>当然还有一些其他的弊端，但是主要的问题就是以上的三个方面。后续在解决这三个问题的基础上，也完善了其他的功能，比如case的执行粒度、case的执行情况统计等等。所以在充分调研了现有的分布式工具的基础上，我们提出了新的需求，新需求主要有以下几个方面：</p>

<ul>
  <li>回归case的时间要尽可能的短，最起码要比现有的执行时间短</li>
  <li>case的执行粒度要尽可能的细，现有框架的case是以文件作为最小的执行粒度，而我们要做的是以caseid作为分布调度的基本粒度</li>
  <li>回归结果的展示要更直观，现有的框架仅能够完成失败case的收集并邮件通知，我们要做的是分布式结果收集、统计、展示、查询这四个维度</li>
</ul>

<p>有了需求我们才开始着手新分布式测试工具的开发工作，后续的章节将会从各个方面进行阐述。</p>

<h5>二、构建Docker Images 以及踩过的坑</h5>
<hr />

<p>对于一个刚开始接触docker的新手，一切都是新的，所以就从最开始说起，说说搭建docker镜像的那些事以及踩过的那些坑吧。。。</p>

<p><strong><em>1、搭建基础docker镜像</em></strong></p>

<p><em>Build docker镜像</em>之前，首先要了解的是你的服务程序是运行在哪个linux版本之下的，是Ubuntu还是Redhat或者其他，尽量选择与现有的平台一致的镜像，如果能具体到linux的版本号就最好了。当然如果你说我的程序是通用类型，具体的版本并不重要，那么选择一个你熟悉的镜像就ok了。比如我的服务端程序是运行在redhat6.5下，那么我会选择Centos6.5的版本作为我的基础镜像。</p>

<p>选择与现有平台一致的镜像有利于后续问题的排查，一般你对服务端程序代码的熟悉程度肯定比不了开发，甚至连一知半解都做不到，如果选择不一致的运行平台，如果出现了问题，你可能无法确定是什么原因导致的，是平台？ or 自有的服务端程序? ,而选择一致性平台则排除了这一问题。</p>

<p>服务端程序以c++为例，都会依赖很多的库，而下载下来的镜像是纯净的系统，所以第一步就是按照程序运行的依赖包。如果开发能给出一个依赖包的列表最好，如果不能，那么尝试着运行程序，将缺少的依赖包记录下来，以便制作dockerfile文件。当你尝试着将所有的依赖包安装完整，并且服务端程序可以正常的启动起来，那么dockerfile文件也就差不多制作完成了。</p>

<p>关于dockerfile文件的基础语法我就不再这里详述了，不懂的 搜狗（这是广告，哈哈…）一下吧。下面是我自己制作的dockerfile文件，或许对你有些帮助</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell"><span class="c1"># Dockfile to install bidding module dependency</span>
<span class="c1"># Based on Centos:6.5</span>
 
<span class="k">FROM </span>centos:6.5
 
<span class="c1"># to replace the origin CentOS-Base.repo</span>
WORKDIR /etc/yum.repos.d
RUN <span class="nb">mv </span>CentOS-Base.repo CentOS-Base.repo.backup
 
<span class="c1"># Add new CentOS-Base.repo</span>
ADD CentOS-Base.repo /etc/yum.repos.d/
 
<span class="c1"># Install dependency package</span>
 
RUN yum install gcc g++ gcc-c++
RUN yum install <span class="nb">curl </span>libcurl-devel
RUN yum install boost boost-devel boost-doc
RUN yum instlal zlib zlib-devel
RUN yum install openssl openssl-devel
 
<span class="c1"># Set CXX and CC</span>
 
RUN <span class="nb">echo</span> <span class="s2">"export CC='gcc -std=gnu99'"</span>    &gt;&gt; /etc/profile
RUN <span class="nb">echo</span> <span class="s2">"export CXX='g++ -std=gnu++0x'"</span> &gt;&gt; /etc/profile
 
CMD <span class="o">[</span><span class="s2">"/bin/bash"</span><span class="o">]</span></code></pre></figure>

<p><strong><em>2、从安装docker到构建完镜像踩过的那些坑</em></strong>
从开始接触到后续构建完成，一路走来，跌跌撞撞，幸好问题都已解决，现就碰到的问题与大家分享一下，或许你也正被其中的某一问题所困扰</p>

<p><em>（1）关于docker安装过程中，离线安装的问题（仅限centos系列）:</em>
docker是可以离线安装的，有离线安装包。如果是离线安装则需要按照cgroup依赖包，<a href="http://mirrors.163.com/centos/6/os/x86_64/Packages/">http://mirrors.163.com/centos/6/os/x86_64/Packages/</a>。在这篇文章中写的比较清楚：<a href="http://www.iyunv.com/thread-149007-1-1.html">http://www.iyunv.com/thread-149007-1-1.html</a>，可根据Centos的版本以及安装包依赖，切记不可照搬照抄。如果是可以上外网，最好还是线上安装吧，比如在Ubuntu下，docker的安装一条命令就可以了：
<code class="highlighter-rouge">
curl -sSL https://get.daocloud.io/docker | sh
</code></p>

<p><em>（2）关于centos6下升级docker引擎到1.2.1版本</em>
docker引擎的1.2.1版本集成了新的swarm，所以如果有可能还是使用1.2.1以后版本的docker。但是对于centos6来说，如果要升级到1.2.1版本，则安装的包需要依赖systemd，但Centos6无法安装systemd。</p>

<p>关于这个问题的回答可详细查看：<a href="http://stackoverflow.com/questions/28347694/how-to-install-systemd-on-centos-6-6">http://stackoverflow.com/questions/28347694/how-to-install-systemd-on-centos-6-6</a>，其中在这篇文章说的在Centos6中安装docker-engine1.2.1，会出现依赖包缺失的错误。<a href="http://blog.csdn.net/weiguang1017/article/details/52293235">http://blog.csdn.net/weiguang1017/article/details/52293235</a>，这篇文章也介绍了如何安装，但是我尝试后还是无法在centos6下无法升级到1.2.1以上版本，如果有成功的同学，麻烦告知，谢谢</p>

<p><em>（3）关于修改devicemapper引擎的相关参数（最大的坑）</em>
当你的docker镜像过大时，比如大于10G，那么默认的devicemapper引擎是会报错的，因为docker service初始化的时候，默认分配的存储空间最大也就是10G，所以必须要修改这个参数，并重启docker service。关于修改这个参数，足足搞了一天，差点都放弃了，因为按照网上的资料所有的更改都无法生效，具体方法如下：</p>

<p>第一种方法为service docker start 之前修改，其修改的内容就是增加–storage-opt dm.basesize=30G选项；第二种方法为调用脚本动态增加，具体的操作方法可以参照如下：<a href="http://blog.chinaunix.net/uid-20788636-id-5029770.html">http://blog.chinaunix.net/uid-20788636-id-5029770.html</a>。</p>

<p>先说第二种方法，在运行脚本的过程中，出现以下的错误：<em>resize2fs: Device or resource busy while trying to open /dev/mapper/docker-253:1-1270544-d2d2cef71c86910467c1afdeb79c1a008552f3f9ef9507bb1e04d77f2ad5eac4。</em>因为此脚本主要是调用resize2f函数来进行动态的扩存，但是由于centos6文件格式的为xfs，对比并不支持，所以此方法行不通。对于centos6来说，还可以统统xfs_growfs来进行扩存，但是对比并不熟悉，研究了一些时间，感觉也没有什么头绪，暂时放弃了。关于resize2f调整rootfs可以参照<a href="http://www.111cn.net/sys/linux/87656.htm">http://www.111cn.net/sys/linux/87656.htm</a></p>

<p>但是对于第一种方法，也是会报错，报错的信息如下：<em>Error starting daemon: error initializing graphdriver: Unknown option dm.basesize</em>。所以两种方法现在都无法达到本来预期的目的，当做一些事情的时候，忽然感觉所有的事情都在阻碍你向前进，这个时候应该静下心，努力排查可能出现的错误，找出其中的蛛丝马迹，应该相信你并非是第一个碰到这个题目的人，合理的利用google。经过很久的排查，终于在github的一个issue里面找到了可能存在的问题，该网页如下：<a href="https://github.com/docker/docker/issues/21171">https://github.com/docker/docker/issues/21171</a>。其中的一位同学做了一下的事情：
<img src="/images/docker/answer.png" alt="" /></p>

<p>再指定storage-opt的时候，同时也指定了storage-driver。抱着试一试的心态，添加了上述参数，果然可以成功的改变container中rootfs中的大小，算做经验教训吧。其他有用的选项可参考如下示例：DOCKER_STORAGE_OPTIONS=”–storage-opt dm.loopdatasize=2000G –storage-opt dm.loopmetadatasize=10G –storage-opt dm.fs=ext4 –storage-opt  dm.basesize=20G”</p>

<ul>
  <li>
    <p>dm.loopdatasize=2000G是指存放数据的数据库空间为2t，默认是100g</p>
  </li>
  <li>
    <p>dm.loopmetadatasize=10G是存放Metadata数据空间为10g，默认是2g</p>
  </li>
  <li>
    <p>dm.fs=ext4是指容器磁盘分区为ext4</p>
  </li>
  <li>
    <p>dm.basesize=20G是指容器根分区默认为20g，默认是10g</p>
  </li>
</ul>

<p><em>（4）关于docker镜像拉取以及squid代理</em>
现在很多公司的机子都是内网环境，无法连接外网，无法连接外网也就无法从docker的registry中拉取镜像，除非你们公司搭建了私有的registry并且包含你所需要的镜像。如果没有，那么有两种途经来解决:</p>

<ul>
  <li>
    <p>第一种方法，就是先在可访问外网的机子上（一般个人本机可以访问外网）下载所需要的docker镜像，然后通过docker save命令将其保存为本地镜像，然后导出后再通过docker load 命令将其导入到所需要的docker宿主机中。这种方法比较麻烦，但是好在一般都能实现，对于确实无法连接外网的宿主机也不失为一种办法。</p>
  </li>
  <li>
    <p>第二种办法，就是在内网中找一台可以访问外网的机子，然后在该机子上搭建squid代理服务器，docker的宿主机可以通过配置代理来拉取镜像。关于squlid代理服务器运行可以参照一下命令:</p>
  </li>
</ul>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell">docker run --name squid -d --restart<span class="o">=</span>always <span class="se">\</span>
  --publish 3128:3128 <span class="se">\</span>
  --volume /search/wangyukun/log/cache:/var/spool/squid3 <span class="se">\</span>
  --volume /search/wangyukun/log/squid_log/:/var/log/squid3 <span class="se">\</span>
  sameersbn/squid:3.3.8-19</code></pre></figure>

<p>当然前提是你已经通过第一种方法安装了squid镜像，这属于一次辛苦多次收益，哈哈，如果你们内网的所有自己都无法连接外网，那么只能通过第一种方法了。启动squid代理服务后，那么就要docker宿主机上配置代理服务，以centos6举例，修改/etc/sysconfig/docker 配置文件的内容如下：</p>

<figure class="highlight"><pre><code class="language-powershell" data-lang="powershell"><span class="nv">other_args</span><span class="o">=</span><span class="s2">"--graph=/search/odin/wangyukun/docker --insecure-registry 10.142.97.235:5000 --storage-driver devicemapper --storage-opt dm.basesize=100G --storage-opt dm.loopdatasize=2000G --storage-opt dm.loopmetadatasize=10G"</span>
<span class="nv">HTTP_PROXY</span><span class="o">=</span>http://your_squid_service_ip:3128
<span class="nv">http_proxy</span><span class="o">=</span><span class="nv">$HTTP_PROXY</span>
<span class="nv">HTTPS_PROXY</span><span class="o">=</span><span class="nv">$HTTP_PROXY</span>
<span class="nv">https_proxy</span><span class="o">=</span><span class="nv">$HTTP_PROXY</span>
<span class="nb">export </span>HTTP_PROXY HTTPS_PROXY http_proxy https_proxy</code></pre></figure>

<p>这样重启docker service 就可以了。镜像搭建以及走过的坑就先说到这吧，剩下的部分第二篇再续。。。</p>


	  ]]></description>
	</item>


</channel>
</rss>
