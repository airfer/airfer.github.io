<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>airfer.github.io/</title>
   
   <link>http://airfer.github.io/</link>
   <description>There are some things in this world will never change and some things do change,everything that has a beginning has an end </description>
   <language>en-uk</language>
   <managingEditor> Wang Yukun</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>2017-2018年度测试总结</title>
	  <link>//summerize2018</link>
	  <author>Wang Yukun</author>
	  <pubDate>2018-04-08T19:28:00+00:00</pubDate>
	  <guid>//summerize2018</guid>
	  <description><![CDATA[
	     <p>
    2017-2018年算是成长比较快的一年，在业务测试、流程推进、自动化测试、持续集成、平台开发、规范制定等方面都有了一些别样的收获。2018年已经远去，记录一下2018年的自己，既往不恋，纵情向前！
</p>

<p><img src="/images/mt/summerize/summerize_mt.004.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.005.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.006.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.007.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.008.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.009.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.0010.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.011.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.012.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.013.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.014.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.015.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.016.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.017.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.018.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.019.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.020.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.021.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.022.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.023.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.024.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.025.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.026.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.027.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.027.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.029.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.030.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.031.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.032.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.033.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.034.jpeg" alt="" /></p>

<p><img src="/images/mt/summerize/summerize_mt.035.jpeg" alt="" />
<img src="/images/mt/summerize/summerize_mt.036.jpeg" alt="" /></p>


	  ]]></description>
	</item>

	<item>
	  <title>2018Q1季度测试总结</title>
	  <link>//sumerize-q1</link>
	  <author>Wang Yukun</author>
	  <pubDate>2018-04-08T19:28:00+00:00</pubDate>
	  <guid>//sumerize-q1</guid>
	  <description><![CDATA[
	     <p>
<a href="http://note.youdao.com/noteshare?id=27988775a0a8c688c4f076d611a62367">2018Q1季度测试总结</a>
</p>

	  ]]></description>
	</item>

	<item>
	  <title>服务端功能测试小记</title>
	  <link>//summerize-for-server-test</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-03-25T17:19:00+00:00</pubDate>
	  <guid>//summerize-for-server-test</guid>
	  <description><![CDATA[
	     <h4>前言</h4>
<p>过年回来之后，业务的功能测试渐渐多了起来，我之前一直负责的是PC方面的测试，而现在除了负责PC的业务测试之外，还负责无线业务的测试。骤然间自己所有的时间差不多都被功能测试任务占据了，到2月份末的时候，关于测试任务的排期都排到了4月初。</p>

<p>在这功能测试的狂轰滥炸中，慢慢的对于服务端重服务的功能测试有了更多的体会，趁着周末空闲的时间整理一下，以飨各位读者。</p>

<h4>功能测试就是手工测试?</h4>
<p>早些时候一直有这样的误区，认为功能测试就是手工测试。现在在脉脉的匿名区还有好多同学在感叹，不想做功能测试了，咨询自动化测试好不好学之类的问题。</p>

<p>在某些同学的潜意识里，认为web端的功能测试就是点点点，服务端方面的功能测试也就是手动构造数据，验证逻辑，这虽然比点点点好上一些，但仍没有跳出手工测试的范畴。对于以上的观点，我个人是不认同的，我认为将功能测试完全等同于手动测试是不恰当的，同样将功能测试与自动化测试完全分开来看也是不合理的。</p>

<p>从我自己功能测试的经验来看，将功能测试转变为自动化测试的一部分是效率最高的一种方法。在阐述这个问题之前，我先大致说一下我之前测试的一般情况。当开发提交测试之后，就根据测试单中的信息，手动构造数据，然后启动服务，验证本次提测的业务逻辑，其实这也是最典型的服务端功能测试的流程。这样做的好处就是可以快速的验证本次提测的业务功能，弊端就是当需要构造的数据量太大的时候，时间的成本也会很高。</p>

<p>除此之外，使用手动构造数据进行功能测试，在多次功能回归的情况下，测试人员是崩溃的，因为开发每修改一些代码，你就要把之前的case都过一遍。PC业务线之前就是这样的做法，先进行手工功能测试，后续抽时间在填充相关的测试case。无线业务线恰恰采用了另一个方法，先抽时间将case写完，然后根据提测需要完善相关case。</p>

<p>在两条业务线实验了一段时间发现，无线业务线所采用的方法，也就是将功能测试变为自动化测试的一部分，效率要高很多。特别是由于一些需求变动，或者少量代码修改，需要验证是否影响之前所测功能的时候，效果尤为明显。这个时候我就让开发人员自己去跑一遍自动化case，而我也从重复的功能性结果验证中解放了出来。这个小主题的意义在于，是否能将现有的功能测试，整合进自动化测试中，当然不同的业务的要求也不一定一致，大家根据自己业务的特点，自行评估即可。</p>

<h4>讨论维护自动化case的必要性</h4>
<p>虽然自动化测试有很多的好处，但是维护自动化case确充满了痛苦，甚至有些时候你恨不得从此再也不用它了。让人如此仇恨的原因，每次跑case失败的太多，而且失败的case大部分是很久之前的功能，很多时候你根本就从来没有听说过这个功能，这种情况下去查看与之相关的case为何失败，我相信很多人面对这种情况，心情都不会太好。</p>

<p>通常情况下，你排查了良久，也无法判断为何某些case失败，郁闷的心情可想而知，这个时候你可能会想，如果只是回归当前提测的功能该是多么幸福的一件事。在经历了多次这种事情后，慢慢的也察觉了一些规律，以及排除某些错误case的方法。就像电视上或者生活中没有无缘无故的爱，也没有无缘无故的恨一样，在自动化case的回归的世界中，也没有无缘无故就失败的case，每一个失败的case都有其失败的原因。</p>

<p>当错误的case发生时，需要排查代码的上一个版本中该case是不是失败。一般情况下，上一个版本的case应该都是全部通过的，因为如果case不通过肯定无法上线嘛。这个时候你就对比当前代码的版本和上一个代码版本，看看究竟是修改了那些内容使得case失败了。通过代码文件静态对比，以及运行期间的gdb单步调试，我想找出失败case的原因不是难事。</p>

<p>经历过多次这样的事情后，就看的比较开了，出现失败的case也会慢慢的去分析原因，不用一出现问题就去喊开发。在这里多说一句，测试人员和开发人员应该保持相对的独立性，不要什么都依靠着开发，如果真的需要找开发来解决某些问题，你应该能大致知道问题出错可能的原因在什么地方。</p>

<h4>如何高效的写自动化case</h4>
<p>谈到写自动化case，很多同学就说，这个很简单，按照EXCEL表中或者xmind图中功能测试的用例，把所有的case都写上就好了。当然这个情况下是最理想的，把所有可能的情况都覆盖掉，但是现实情况下，你可能根本没有时间将所有的case全部写完，这个时候你就要在规定的时间内，用最少量的case完成最大的代码覆盖，拒绝重复的case，以及一些非常简单的case。</p>

<p>重复的case这个比较好理解，比如某项功能在某个测试用例中已经验证过一次了，那么就没有必要在其他的case中再验证一遍。那么什么是简单的case呢？说到简单的case，就要提及代码review了，现在很多测试不参加开发的代码review，当然各种原因都有，比如时间紧、任务重啊，或者没有这样的惯例啊等等。</p>

<p>我想说的是，如果有条件，尽量在进行完粗略的主功能验证后，开始进行代码review，代码reivew不但可以让你对所测业务理解加深，提前发现一些代码级的bug，对于编写自动化测试用例也是益处良多。比如代码中，有关于某些字段的验证，再仔细查看代码后，针对性的构造自动化case，没必要根据每一个字段分别构造case，甚至你通过查看代码某部分业务逻辑已经非常清楚了，在时间紧的情况下，可以不添加与之相关的case。</p>

<p>简单的case是建立在你对代码逻辑异常清楚的情况下，判断某业务逻辑非常简单，不值得添加用例进行覆盖的case。恩，比较绕口，但应该不难理解。</p>

<h4>框架的易用性、通用性以及高效执行</h4>
<p>当case添加完成之后，总体回归所有case时，一般为了节省时间会将case分发到多台主机上同步执行。当case的数量巨大时，这种设计思路非常有必要，以目前的无线的自动化case举例，现在差不多接近600个case，如果放在单台机子上跑的话，跑完要1个半到2个小时。</p>

<p>如果分散到三台机子上跑，可能半个小时就跑完了，case数量的不断增加，分布式执行成为必要。关于分布式构建之前已经写过文章分享过，这里就不再过多阐述了，有兴趣的同学可以自己找来看看。</p>

<p>本小节要重点谈的是框架的易用性、通用性和高效执行。易用性很好理解，就是上手非常快，只需要填写少量必须的参数，整个任务就可以跑起来了，想目前一直在使用的第一版基于jenkins的分布式系统，只需要填写本次代码的svn地址或者bin文件，然后根据功能需要在中控机上做少量修改，就可以执行了。因为上手非常容易，所以教开发自己来跑任务也不用花很多时间成本。</p>

<p>我之前设计的第二版分布式系统，解决了易用性和高效执行两个方面，但是在通用性上做的不好，所以到现在就pc业务线在用，甚至我想把无线的分布式迁移过来，也不是几行代码或者几个小时能搞定的。由于现有的框架和业务联系太紧密，导致了扩展性也不好，现在正在开发基于ansible和docker的分布式解决方案。关于这个方案我会在后续的文章中详细的谈，这里就不多说了。特别是在大部门，多业务线的情况下，一个框架的设计要兼顾几个方面，能复用就复用，不要重复的开发，浪费人力物力。</p>

<p>高效执行之前也谈到过了，就是如何在尽可能短的时间内，将程序运行完。第一版的分布式系统虽然利用分布式主机的特点，提高了执行时间，但是还是没有把现有的计算机资源利用起来。第一版的分布式系统，单主机情况下，同一时间只有一个服务实例在运行，而现有的主机资源，可以同时支持3个甚至更多的运行实例，所以第二版分布式系统和第三版就利用docker 容器规避了这个问题。对于一个通用性的工具或者框架，以上三个因素都非常重要，如果不能兼顾的情况下，根据业务需求自行取舍吧。</p>

<h4>结语</h4>
<p>啰啰嗦嗦说了这么多，主要是我自己的功能测试感悟，可能对某些方面的理解有些偏颇，大家不必较真，毕竟对于同样的问题，每个人都有每个人的看法。如果能从文章中感悟一点有益的东西，也不枉费了我周末码字的辛苦。同样欢迎有不同看法的同学留言交流……..</p>


	  ]]></description>
	</item>

	<item>
	  <title>年终总结2016,写在农历鸡年之前</title>
	  <link>//summerize-2016</link>
	  <author>Wang Yukun</author>
	  <pubDate>2017-01-27T10:19:00+00:00</pubDate>
	  <guid>//summerize-2016</guid>
	  <description><![CDATA[
	     <h4>前言：</h4>
<p>一直想抽出时间来把这一年所做的事情总结一下，但是每次开始写的时候又不知道该从何写起，毕竟一年经历了很多的事情。为了不在记述的时候产生跳跃感，我就按照时间线的顺序写吧，如果要先有一个概述的话，那2016算是稳中有升的一年~</p>

<h4>前半年：</h4>
<p>2016年的前半年，主要还是在老东家360中渡过的。那个时候我从360支付平台转到360广告平台也有一段时间了，对广告平台的理解也慢慢的变深，主要负责的是360广告平台的front端的测试。测试的内容起先负责包括广告的展现打点、计费打点、打点信息校验、广告展现兼容性测试等内容，后来也负责了一部分eapi的接口测试、kafka消息队列相关测试等内容。</p>

<p>在测试的过程中对广告平台的理解也逐渐深入，其实这相当于是一个打基础的阶段，毕竟相关业务的测试都是在这个基础之上的。测试的主要方式还是手动人工测试，除了看日志之外，很多功能的测试都是需要去手动完成，例如点睛投放平台相关测试，基本上靠人工，由于投放平台的界面变化较大，做UI自动化成本太高，任务又急，这个时候也只能靠人了。</p>

<p>当时我的leader是萍姐，反正从我转入广告平台后，一直都是萍姐带着我熟悉广告平台的很多东西，萍姐是一个很称职的leader。即使后来我要离开公司，其实对萍姐还是有很多不舍，最后走的时候萍姐对我说的话，我现在也铭记着。<em>其实在工作中遇到对自己有帮助的人都是应该感恩的，毕竟并非每个人有义务对你好，给你指导，在工作中遇到一个好的leader真的是一件很幸福的事情</em>。扯远了，其实除了leader的指导帮助之外，更多的是靠自己去实践和领悟，别人教你的东西始终是别人，想把这些东西转变成自己的，只能靠自己多实践、多想、多问！</p>

<p>我对广告平台认识加深的过程，其实是在测试凤舞项目的时候，由于这个项目牵扯到方方面面，包括广告平台的物料投放、eapi接口、消息队列、searchfront引擎前端、query检索端、index检索生成、material物料库、广告展现、front打点等等。所以当这个项目测试完毕，我对于广告平台的认识也变得更加的深入，所以实践出真知不是没有道理的。但是由于引擎端的代码并不对测试人员开放，所以引擎端的处理逻辑对测试人员是黑盒，这在一定程度上也阻碍了测试人员的进一步深入。</p>

<p>其实前半年在点睛业务团队，也做了很多自动化的东西，包括打点的网页端展示系统、基于Nodejs的轻量级日志监控系统等，原本打算基于ELK技术栈来改进现有的日志监控，由于后来离职了，只对交接的人员大致讲述了下思路。其实到新公司之后，我也了解过我做的这些工具的情况，状况很令人心碎，这些工具无人维护基本上死掉了。我觉得这个状况在各个公司都存在，很多人觉得维护旧的工具还不如直接做个新的，重复的造轮子。后来想到之前的教训，我在现在的公司才更多的强调，开发工具的稳定性以及可维护性。我的目标是即使我离职了，我开发的工具也还可以运行N年。。。</p>

<p>总之,2016前半年更多的收获是在对广告平台的认识和理解，收获的是即使是做手工测试人和人之间的差距也可以很大，收获的是测试工具的根基在于业务，对业务了解不深，为了做工具而做工具得到的只能是失败。在离开360的时候，我自己的目标就是做一名顶尖的功能测试工程师。收获有很多，当然也有很多遗憾，遗憾的是对某些业务的深入程度还不够，比如ESC统计计费，还有就是没有加强自己的开发能力，业务虽然很重要，但是开发能力也很重要，不要顾此失彼。</p>

<h4>后半年：</h4>
<p>从老东家360离开之后，便来到了现在的公司，搜狗。其实之前对搜狗并不是很了解，我影响最深的产品就是搜狗输入法，其他的真不是很清楚，后来面试之前还好好的补了下课。来搜狗也有一定的缘分吧，记得有一次，我现在的leader ZZ,在脉脉上和我随便聊了几句，感觉这个leader挺好的，后来就约了个时间过来聊聊，后来就来了搜狗，哈哈。我在搜狗这边也主要负责的是广告这块，只不过负责的内容对我来说正好是我之前不太了解的，之前在360的时候，负责的是搜索引擎之上的测试，比如广告投放平台；在这边主要是负责搜索引擎本身，主要是广告内容的检索、加载、展现、屏蔽等内容。这恰好是对我缺失的部分进行了一个弥补，毕竟在360，我也接触不到引擎级的代码。</p>

<p>刚开始接手的时候还是有难度，毕竟我之前也没接触过。刚开始最让我吃惊的是，竟然在1个月之后要进行代码串讲，相当于要在1个月的时间对引擎的代码读一遍，然后理出思路，这对刚接手的我绝对是个不小的考验。除了看之前关于引擎的笔记之外，我师傅军哥，还有佳佳，都帮了我很多，毕竟要想在较短的时间内掌握更多的东西，还是要多向前辈们请教，搜狗的同事都很nice，你只要问，同事都会乐于告诉你。</p>

<p>代码串讲之后，也就开始慢慢的接各种业务提测，在测试过程中就会发现，在熟知代码后，测试的逻辑也变的异常清晰。源码面前无秘密，这次我是深刻体会到了。测试也参与开发的代码review，确实让测试快速的成长起来。熟悉了基本的业务后，就可是熟悉各种已经存在的测试工具、系统，我接手的时候这些测试工具都维护的比较好，主要是分布式测试系统，以及压力测试系统。每次测试完功能后，将这两个功能都跑一篇，进一步查漏补缺，有好几次bug就是在查漏补缺的过程中找到的。</p>

<p>我慢慢的体会到了，分布式系统以及压测系统的重要性，后来也慢慢的接手维护的工作。到现在为止，我们每次跑的全量case数据大约在540条左右，所有的提测任务完成后，在一定的时间内就会把case添加到case集中，这形成了一个正向的循环，测试人员也慢慢的从重复的功能性回归中解放出来。现在的分布式测试系统，以及压测系统，开发也会自己去跑，我们现在所做的慢慢由之前的一次性交付向测试服务provider转变。这是一个很好的现象。</p>

<p>到了现阶段的话，除了维护测试工具的稳定性，完善case集之外，更多的是对现有的工具进行改进，提升测试效率，提高生产力。为了缩短分布式系统的执行时间，我们引进了docker，使用docker容器来作为基础单元参与调度，所有case的执行时间由原先的20多分钟缩短到5分钟。与此同时，为了管理docker容器，我们使用了swarm以及shipyard来进行基础容器编排以及拓展，并结合ansible来提升管理的效率。在压力测试中，我们正在整合现有的压测工具，使之平台化、通用化。</p>

<p>除此之外，我们还开发了新版的广告预览平台，开发测试都在使用，其实由一次性交付向持续交付转变，最多的障碍其实是在思维上。不管怎么样，我们都是在不断的改变自己，不断的利用新的技术、方法、思维来提高测试的生产力，这个方向总归是对的。</p>

<p>除了技术方面，如何让别人来分享自己的研究成果也是很重要的一方面，之前都是规定每个人都要进行分享，到后来很多同学觉得这变成了一个负担，分享的质量也有所下滑。分享结束后，如果没有参加分享的人可能只能通过看ppt来大致了解所讲述的内容，所以单纯的讲座效果并不如想象的好。现在部门内成立了技术影响力小组，老大让我负责，其实我有些诚惶诚恐，毕竟我自己的技术也很一般，只希望自己能近最大的努力把这件事情做好。大致弄了一个大纲，后续和leader去对一下，然后慢慢的把这件事做起来。</p>

<h4>结语：</h4>
<p>时间过的真的很快，一转眼2017已经来了。2016年对我来说是成长和收获的一年，希望自己和部门能够在新的一年里越来越好！2017，加油干，撸起50亿！！！</p>

	  ]]></description>
	</item>

	<item>
	  <title>测试总结之述职杂谈</title>
	  <link>//summerize-report</link>
	  <author>Wang Yukun</author>
	  <pubDate>2016-11-26T10:18:00+00:00</pubDate>
	  <guid>//summerize-report</guid>
	  <description><![CDATA[
	     <h4>前言:</h4>

<p>最近一段时间一直在忙着平台开发的一些事情，一直都没有时间梳理和总结这过程中的点滴。下个月要准备述职报告，所以就借着这个机会把测试过程中的一些感受记下来，以飨各位看官。本篇是杂谈，所以不谈具体技术，只谈风月…..</p>

<h4>一、论工具之稳定性压倒一切</h4>

<p>作为一名测试开发工程师，测试工具应该是非常熟悉了，测试工具是测试过程中的好帮手。好的测试工具对于测试人员效率提升以及减少漏测率都有非常大的帮助。测试工具的来源也有很多，有的工具是较为成熟可以直接拿来用的，有的是基于一些开源的项目然后根据自我需求二次开发的，还有一些是对于某些需求特殊定制的等等，那么对于测试工具什么最重要呢？</p>

<p>从标题可以看出我的结论是，稳定性是一个测试工具最核心的需求。当然这并不是说测试工具的执行效率，测试工具的可扩展性以及可维护性不重要，只是相对于这些特征，稳定性才是一个工具必须要保证的特质。</p>

<p>从半年前的入职到现在，搜狗ADTQ测试组给我留下较深印象之一便是持续集成做的非常出色。因为我所在PC组，所以就以PC组举例。现在PC组对于新增的提测单，除了人工的验证新增功能以及样式展示外，其他的全部交给自动化来执行，包括所有case的分布式执行测试、新旧版本的压力测试、新旧版本的对比测试等等。其中令我吃惊的是这些部署在jenkins上的任务，大多是在2014年就已经完成开发，从2014年到现在一直都在稳定的运行，除了进行必要的维护以及case更新之外，这些工具都表现的非常出色。</p>

<p>这些工具大部分是东哥（暂且这么叫吧^^）开发的，主要通过shell脚本来实现，我和东哥也没见过我入职时东哥已经离职了，但这也不妨碍其所做的工具对我们目前工作的帮助。以case的分布式测试为例，其实现分布式也并非用了非常高深的技术，主要是通过jenkins的api以及shell来完成，就这个看似简单其实并不简单的工具支撑着我们每次上线前的500多个case的回归，帮助我们减少漏测的情况。只要case的分布式执行出现问题，比如突然失败几十个，有可能是功能的漏测，也有可能是合并后的代码有误。</p>

<p>这个分布式case的执行工具之所以可以用的现在，其本身程序的稳定性占有很大的比例，如果一个工具每次的运行都不稳定，或者运行的时候时不时的报个错误，出现个exception，你还会信赖它？还会用它吗？</p>

<p>我所希望的工具就是在我需要它的时候，它能够按时完成我交给它的任务，别给我出乱子，这就足够了。我最讨厌的测试工具就是我一边在使用着这个工具，一边忍受其所报的各种错误提示，对于出现的错误我还要调试半天究竟是什么地方出错了。就是这样一件非常简单的要求，又有多少工具能够很好的满足呢，所以我才要特别强调工具稳定性的重要性，我希望即使我以后离开公司，我的工具也可以在仅有必要维护的情况下能够很好的工作。</p>

<p>最近一段时间做了两个内部使用的平台，其中一个就是开发了新的分布式测试系统，改进了原先分布式测试系统的一些缺点。在试运行阶段，修改了各种bug，现在已经上线并取代原有的分布式测试工具。在开发新的分布式测试系统时，我就感觉到要做好一个工具的稳定性真的非常不容易，需要考虑各种可能的情况，比如一些特殊的输入，执行过程中可能出现的异常，还有case执行过程中的一些特殊的依赖等等。</p>

<p>希望新的分布式测试平台能够发扬老一辈分布式测试系统的优点，将稳定性的核心特质保持下去。这里感谢下东哥，在设计新的测试系统时借鉴了很多老平台的优点。</p>

<p>其实当设计平台将稳定性考虑在内时，你就会考虑更多的东西，比如你的系统需要依赖机子A上的某个文件，如果这种依赖关系是硬编码在代码中，那么极有可能出现A机子文件被删除导致获取失败的情况。这种情况下如果是你自己在维护可能还好，如果你已经离职，新入职的员工如何知道你所依赖的这个文件是什么内容，这种依赖关系是否是必须，这些新员工都不清楚。</p>

<p>如果这种依赖关系是可配置的，不管是通过xml还是json或者web界面，并对这种依赖关系进行详细标注，甚至说明所依赖文件的生成方法，那么这种系统的稳定性以及可维护性就要好很多，甚至不需要去读测试工具的代码就能搞定所出现的问题。关于稳定性还有很多的要求，由于我自己也处在学习阶段，所以就先谈到着吧</p>

<h4>二、论旧工具与新工具的前世今生</h4>

<p>旧工具可以理解为早些时候开发的工具，这些旧的工具有些可能还在使用，有些可能因为年久失修已经废弃了。当需要这些测试工具所提供的功能时，是继续使用这些即使有些缺点但是还可以使用的旧工具，还是花费大量时间精力去开发新的工具，我相信很多测试同学都有过这种选择，在综合权衡下，做出自己的选择。</p>

<p>我知道肯定也有一些同学与我的想法一致，旧的工具用着不顺，新的工具开发成本又太高，那么就综合两者，以旧工具为蓝本添加新的功能修复原有工具的缺点。当然这三种选择并没有孰优孰劣，能根据具体的测试需求综合权衡，做出正确的选择就好。比如原有的工具实在是太难用，而这个工具开发有比较简单，那么可以考虑重新开发一套，具体情况具体论，切勿盲目追从。</p>

<p>本节所谈论的主要是第三种情况，对现有旧工具进行改造，要不怎么说是前世今生呢，哈哈。还是拿我自己举例吧，现在组内的一些工具，比如性能对比工具，其实现起来比较复杂，首先便是需要搭建一套测试环境，然后再进行对比分析，现有旧的工具也还可以使用，像上一节所谈的，稳定性很好，但就是有些缺点比如生成的报告不直观、调用入口单一（大多数情况通过jenkins平台直接调用）、出错排查不方便等。</p>

<p>如果综合权衡考虑这几方面的因素，会发现改进原有旧工具的缺点，并添加所需要的功能是效率最高的。由于现在ADTQ组有一个统一的测试平台，整合了测试组常用的工具，所以考虑是否能够将原有的jenkins任务整合进现有的平台当中，使得调用更加方便，结果展示更加人性化，错误排查容易呢？通过查询资料以及对旧平台代码的通读，发现改造是可行的，是可以实现的。</p>

<p>由于现阶段还没有进行改进开发，所以就大致说一下我自己改进的思路吧：
1. 关于任务的执行、查询、终止等操作全部通过python版本的jenkins api来完成；</p>

<ol>
  <li>
    <p>修改原有的jenkins任务脚本，添加结果收集模块，将收集的结果写入到数据库中，mysql或者Mongodb都可以；</p>
  </li>
  <li>
    <p>重新开发前端页面，主界面主要有任务触发、结果查看等功能。</p>
  </li>
</ol>

<p>其实这个思路也并不复杂，实现起来的成本也不是很大，主要的工作在于前端页面设计以及和数据库的交互逻辑，这样的改进方法比起重新开发新的工具要省事省力很多。当然也有其他的方法，比如组内的宋大神（非常NB）就利用异步框架Celery加paramiko来实现对脚本的调用，以代替jenkins的工作。改造的方法多种多样，可爱的同学发挥你的聪明才智吧~</p>

<p>旧工具与新工具并非是势同水火的关系，如果能让旧的工具进一步贡献自己的价值那么何乐而不为呢。可能有的同学会问，既然比较推崇对旧工具进行改造，那么你为何又重新开发新的分布式测试平台呢？</p>

<p>其实在本章节我有说过，做出某项选择必然是要经过充分的权衡考虑的，我这里解释一下，旧的平台主要有几个明显的缺点比较难以克服所以才考虑开发新的平台，其一是执行的粒度无法控制，其二是分布式节点部署非常繁琐极易出错。这两点我开发新平台的主要因素，现有的平台在节点部署方面做的非常出色，通过ansible可实现1分钟内将分布式节点由3个扩展到N（具体多少看ansible的性能）个，而旧的平台要实现这样的改进就非常困难，其成本不亚于开发新的平台，所以才考虑开发新的平台</p>

<p>前段时间开发的广告预览平台也是放弃了旧的平台，开发新的平台。之所以做出这样的选择，并非是对旧的平台改造困难，恰恰相反，由于预览平台的技术含量较低，并且有现成的api接口可供调用来完成图片获取，开发的成本较低，同时为了界面更加的友好所以才重新开发。举了这些例子就是想说明，究竟采取那一种方法是需要根据具体情况具体分析的，不能因为某些炫酷的功能而盲目开发新平台，同时也不能因为开发新平台或者改进旧平台的成本太大就放弃努力，得过且过。其实大家都知道我说的是什么意思，这里不再赘述了</p>

<h4>三、脚踏实地、仰望星空</h4>

<p>温家宝总理说过这样的话：”一个民族有一些关注天空的人，他们才有希望；一个民族只是关心脚下的事情，那是没有未来的”。温家宝总理说这句话时，想表达的是一个国家即需要踏实肯干的人才，也需要有卓越远见，考虑长远的人才。</p>

<p>这句话放在部门内也同样行的通。在当今的社会我们既要做到踏实肯干，同样也需要能跳出当前着眼长远，不要故步自封。如果只是抱着旧的工具方法、旧的流程规章，虽然短期之内并没有大的坏处，但是长久来看，落后是必然的。所以如果有新的想法、新的设计构想、新的技术就要大胆的尝试，虽然不能保证每次尝试都有好的结果，但是如果连努力尝试的想法都没有，一个部门想保持长久的活力也是不可能的。</p>

<p>还是举个例子吧，现在部门内的无线端广告预览只有模拟版本，也就是分辨率可以确定，通过phantomjs来截图。其实都知道如果能实现真机的截图就最好了，考虑很多方面以及testerhome上分享的文章，我们觉得使用stf或许可以完成这样的功能，虽然最后的结果不一定能成，但是尝试又不收费，再说万一要是成了呢。现阶段各种新的技术、新的方法层出不穷，如果能充分利用现有的新技术、新方法，或许可以做到事半功倍。</p>

<h4>四、论错误排查</h4>

<p>之所以把错误排查单独列出来，是因为我觉得错误排查的能力真的非常非常重要。不管是你自己的程序还是测试工具或者测试环境甚至开发的程序，如果出现了问题，应该知道如何去排查。</p>

<p>现在有些测试同学，都没有自己的测试环境，测试和开发共有一套环境，难道测试的地位真的这么低，都不能有一套独立的测试环境。其实有些时候并非是因为测试地位低，而是测试自己不想去维护这套环境，习惯了只要环境出问题就喊开发来解决，并且认为测试的环境理应当由开发来维护，真不知道为何有这样的想法。如果连维护一套测试环境的能力都没有，还谈什么自动化测试。</p>

<p>我始终相信每个错误的产生必然有其产生的原因。如果一个错误出现了，那么在程序以及资源不发生变化的情况下，其重复出现是必然的。之前出现错误会感到困惑和无奈，而现在则慢慢学会了抽丝剥茧，探索错误产生的根本原因，其实错误排查的过程也充满了乐趣。当然并非每次都是这样，我自己就被一个错误折磨了差不多一个星期，其实差不多都快到崩溃的边缘的，所以耐心也很重要。</p>

<p>举个例子吧，在新的分布式测试系统的实现过程中，就出现了这样一个问题，在docker 节点中通过nosetests来执行case时，有一定的概率server主程序会被杀死，重点就是在这一定概率上，因为对于同一个case有时候server运行是正常的，有些时候则直接被kill掉。我反复将相关的shell以及python脚本通读好多遍，都觉得不应该出现这样的问题。就在我差不多快崩溃的时候，才发现原来node节点中部署了一个case收集程序，这个程序在收集case时会杀掉当前的server程序，由于这是一个crontab任务，所以这才导致了server被杀死的概率性。当时找到这个错误的原因时，真的蓝瘦香菇了。。。</p>

<p>再说一个例子吧，通过docker来部署stf服务的时候，所有都部署完毕，但是stf主程序访问就出现错误，但是stf local程序就可以好好的运行。后来也是经过详细的排查才发现由于stf 容器在vmware虚拟机中运行，而其中的ubuntu虚拟机是采用NAT的方式，这导致了docker0在此网络模式下无法与host进行通信，所以导致无法访问。</p>

<p>举得两个例子是我自己错误排查过程中遇到的两个印象较深的错误，但是好在都找到了原因，其实在错误排查的过程中，你对某程序某方法必要要有很深的了解，要不你怎么排查呢？这样反过来也加深了对某些知识的理解，既解决了问题又涨了知识，这或许是对错误排查过程中所遇到困难的补偿吧。测试同学不要一有问题就找开发了，锻炼一下自己的排错能力吧。。。</p>

<h4>五、总结</h4>

<p>差不多也就这样了，到今天入职也半年了，点点滴滴都记在心里，整理整理自己的收获、感受、所学也是对过去自己的一个交代。下面是之前发在论坛上的几篇文章，有兴趣的同学可以抽空看看，就到这吧</p>

<ul>
  <li>
    <p><a href="https://testerhome.com/topics/5669">基于 Gtest 的单元测试入门及实践 (一)</a></p>
  </li>
  <li>
    <p><a href="https://testerhome.com/topics/5675">基于 Gtest 的单元测试入门及实践 (二)</a></p>
  </li>
  <li>
    <p><a href="https://testerhome.com/topics/6184">基于 Docker 的分布式测试系统构建 (一)</a></p>
  </li>
  <li>
    <p><a href="https://testerhome.com/topics/6187">基于 Docker 的分布式测试系统构建 (二)</a></p>
  </li>
</ul>

	  ]]></description>
	</item>

	<item>
	  <title>发布在脉脉上的一些文章</title>
	  <link>//articles-published-in-maimai</link>
	  <author>Wang Yukun</author>
	  <pubDate>2015-08-25T14:18:00+00:00</pubDate>
	  <guid>//articles-published-in-maimai</guid>
	  <description><![CDATA[
	     <h4>前言</h4>

<p>测试做的久了，可能会对测试本身有很多的看法，本篇算是对测试理念以及测试技术的个人总结及感悟吧。当谈到对测试的理解可能每个人的理解都不相同，这和个人所从事的业务测试，以及自己在测试中形成的固有观点有关，不能直接的就评判这种测试理念是正确的，而其他的测试理念都是错的。   </p>

<p>测试理念是指导测试的准则！测试理念的好坏是需要经过实践来检验的，实践是检验真理的唯一标准，这一条也同样适用于测试。当你将相关的测试理念用于某些业务级测试，能够提高开发的产品质量，对质量保障起到很好的作用，没有明显的弊端， 可以说这个测试理念对这项业务是有效的，是经得住检验的。但是不是这套测试理念，对于某业务级的测试是最优的呢，这个暂时还不能下结论，需要在实践中反复的去尝试</p>

<p>下面就浅谈一下相关的测试理念以及测试技术吧</p>

<h3 id="section">一、测试理念杂谈</h3>

<p><strong><em>1、简述对测试开发的理解</em></strong> <br />
测试开发，首先放在前面的应该是测试，脱离了功能性测试就妄谈自动化测试、接口测试、性能测试等等都是扯淡。如果不懂业务逻辑，如何做到自动化测试的精确覆盖，如何理清接口测试中的调用逻辑，如何知道性能测试可能的瓶颈？  <br />
…………  <br />
<a href="https://maimai.cn/article/detail?fid=38942468">查看全文</a></p>

<p><strong><em>2、接口自动化测试的痛点在哪里</em></strong> <br />
现在发现脉脉的专栏还真是挺好的，自己的思想有对有错，大家相互交流可能知道误区在哪里，对于理解错误的地方请大家轻拍，大家轻拍，大家轻拍，我说了三遍了。与其说是大家相互交流探讨，其实更像是我自己的总结。   <br />
………… <br />
<a href="https://maimai.cn/article/detail?fid=38970700">查看全文</a></p>

<p><strong><em>3、喜欢这个看实力向”钱”看的社会</em></strong> <br />
最近几天各个公司的调薪都出来了，涨的多的自然很高兴，涨幅不及预期的可能想着找下一个东家了吧。  <br />
…………  <br />
<a href="https://maimai.cn/article/detail?fid=39018511">查看全文</a></p>

<p><strong><em>4、单元测试去哪儿了？</em></strong> <br />
从入行测试以来，对于单元测试了解甚少，现在在公司所接触的更多的是功能、是接口、是性能，鲜有单元测试的大牛分享单元测试的经验。有时我会想为什么专业的单元测试人员比较少？是大家都去做上层的测试以期达到对底层的覆盖，还是产品版本迭代速度过快导致没有足够的时间，再或者是由于我身处的地方，身处的业务不适合做单元测试？从我的角度来看，单元测试去哪儿了呢？   <br />
…………   <br />
<a href="https://maimai.cn/article/detail?fid=39079504">查看全文</a></p>

<p><strong><em>5、我工龄短又怎么了？</em></strong> <br />
现在的企业招聘一般都会设定应聘者的工作年限，比如3-5年或者更久5-10年的也有，那么工作两年的是不是就没法去应聘了？当看着招聘需求对应聘者工龄设卡时，我都是一脸懵逼的在想，我工龄短又怎么了？   <br />
…………  <br />
<a href="https://maimai.cn/article/detail?fid=39116695">查看全文</a></p>

<p><strong><em>6、浅谈广告平台系统架构</em></strong> <br />
之前写的几篇文章，大部分都是关于一些测试理念以及对某些问题的看法，写的多了难免有发软文骗眼球的嫌疑，今天就来点干货吧，借着小长假的时间顺便将之前广告测试的一些经历以及感受给大家分享一下，或许对一些从事相关业务的同学有些帮助以及启发。    <br />
…………  <br />
<a href="https://maimai.cn/article/detail?fid=39205014">查看全文</a></p>

<p><strong><em>7、浅析支付业务流程与实现</em></strong>   <br />
在接触广告业务之前，最早接触的其实是支付业务，负责360支付平台整体测试，更多的是对服务端底层进行功能测试以及接口测试。也可以说我的功能测试基础是从哪个时候搭建起来的。   <br />
…………   <br />
<a href="https://maimai.cn/article/detail?fid=39292100">查看原文</a></p>

<p><strong><em>8、说说性能测试的那些事</em></strong>  <br />
作为一名测试开发人员，特别是服务端方向的测试开发，有一个领域是没有办法绕过的，那就是性能测试。有些公司会专门去招募一些性能测试工程师，专门就某业务进行测试，但是大部分的公司的性能测试都是由普通的测试开发人员来做，甚至有些时候，开发觉得较为麻烦，可能就自己自测了。   <br />
…………  <br />
<a href="https://maimai.cn/article/detail?fid=39388093">查看原文</a></p>

<p><strong><em>9、除了业务与技术，测试开发还需要啥</em></strong>  <br />
刚接触测试开发的工作，可能将重点放在业务的理解以及技术的提高上，这一点并没有错，因为对业务的理解有助于功能测试，而技术的提高有助于自动化测试。随着时间的推移，如果仅仅只关注于这两个方面，可能会让自己存在短板，影响自己的后续的职业发展，问题来了，除了以上的两个方面还需要学什么？   <br />
…………   <br />
<a href="https://maimai.cn/article/detail?fid=39493625">查看原文</a></p>

<p><strong><em>10、静态代码扫描在 360 无线项目中的实践</em></strong>   <br />
之前写的文章都是更多的是偏向于服务端测试方向。而我本人也对无线方向实践的并不多，更多时候是学习一下自动化测试框架，比如robotium、appuim等。今天主要是介绍一下360公司内部的一款静态代码扫描工具，我自己试用过，非常的不错，希望这款工具能给大家带来一些启发。   <br />
…………  <br />
<a href="https://maimai.cn/article/detail?fid=39641600">查看原文</a></p>

<p><strong><em>11、再谈对接口测试的思考与实践</em></strong> <br />
与接口测试相关的之前已经谈论过一篇，只是相关测试理念的阐述，这篇就详细一点吧。当谈到接口测试，肯定是要选择接口测试框架或者工具，不管是现有的，还是自己编写的。现在大家都倾向于自己去造轮子，可能并不知道开源项目中已经有一些很好的实现，就拿Postman来举个例子吧，熟悉接口测试的应该对此工具不陌生，如果你并不知道Postman，可以事先去了解一下，真的是一个不错的工具。  <br />
………… <br />
<a href="https://maimai.cn/article/detail?fid=39689813">查看原文</a></p>

<p><em>**12、论测试策略与业务的相关性 **</em>  <br />
前一段时间和一些测试leader聊天，感觉自己也收获了很多，每个测试的leader对其所负责的业务都有自己的一套测试指导策略。它相当于是测试某项业务的总纲，来指导如何对业务进行测试。其实谈这些感觉自己的资历还是尚浅，这篇就简谈一下自己的看法吧，因为没有做过leader很难就具体的某些策略细节展开详述，大家也就随便看看吧    <br />
………… <br />
<a href="https://maimai.cn/article/detail?fid=40528821">查看原文</a></p>

<p><strong><em>13、欢乐颂中的邱莹莹和曲筱绡相比差在哪？</em></strong>  <br />
最近几天朋友圈以及网上被欢乐颂刷屏了，周末正好抽着时间看了看，感觉真的是一部非常不错的电视剧。看的过程中对剧中饰演的邱莹莹和曲筱绡留下了很深刻的印象，试着说说自己对两个人的看法吧。 <br />
……………  <br />
<a href="https://maimai.cn/article/detail?fid=41637293">查看原文</a></p>

<p><strong><em>14、关于ELK技术栈在实际应用中的思考</em></strong>  <br />
在五一之前就想总结一下ELK实践的思路，但是由于各种原因搁浅了。最近几天梳理了一下ELK方面的资料，把之前理解比较模糊的地方重新看了一遍，感觉实践的思路更加清晰了，在自己的心里能把整个框架串起来，这应该算是较为粗浅的入门吧。  <br />
…………   <br />
<a href="https://maimai.cn/article/detail?fid=43970562">查看原文</a></p>

<h3 id="section-1">二、关于测试中的一些注意点</h3>

<p><strong><em>1、分工问题</em></strong></p>

<ul>
  <li>首先测试并非只是测试工程师的事情，测试也应该根据测试工程的情况，让开发或者产品灵活配合</li>
  <li>虽然有分工，但是也应该有自主意识，明确哪些是测试重点提高开发的自测水平</li>
</ul>

<p><strong><em>2、测试可大可小？</em></strong></p>

<ul>
  <li>如何区分这是一个小的测试还是大的测试呢？有人说根据参与人的多少来判定，有一定道理但不可靠</li>
  <li>首先要评估改测试设计的影响面、影响的深度，以及其实现的复杂程度，这是多方面考评后得到的结果</li>
  <li>这个测试要不要测试或者能不能测试的问题，要依据具体情况而定，如果某场景在线下确实无法模拟，那么只能在线上进行回归测试</li>
</ul>

<p><strong><em>3、是不是只要线上出问题都应该是测试人员负责呢？</em></strong></p>

<ul>
  <li>可以分为两个部分，和产品以及开发明确。正常测试部分以及免责条款</li>
  <li>正常部分：正常的测试流程、以及各功能模块的线下测试、线上回归</li>
  <li>免责条款：主要由开发人员进行质量保障，测试只能进行简单的回归工作</li>
  <li>说明：至于正常部分以及免责条款的制定，需要根据项目的实际需求，综合项目之前的提测来进行制定</li>
</ul>

<p><strong><em>4、对业务的环境敏感吗？</em></strong></p>

<ul>
  <li>测试人员并非只是进行业务的测试工作，还应该多关注业务发展的环境情况</li>
  <li>环境情况包括但不限于以下内容：</li>
  <li>业务的分支</li>
  <li>业务的整体运维情况</li>
  <li>业务的收入性来源，以及各业务收入的比重</li>
  <li>业务获取公司资源的倾向性</li>
  <li>负责业务的开发或者产品的信息共享渠道比如wiki等</li>
  <li>业务的人员流动变化</li>
</ul>

<p><strong><em>5、如何进行信息的共享操作？</em></strong></p>

<ul>
  <li>svn、git、wiki</li>
  <li>说明：是否有ACL限制等</li>
</ul>

<h3 id="section-2">三、总结：</h3>
<p>希望一些测试理念或者方法对看到这篇文章的你们有帮助！</p>


	  ]]></description>
	</item>


</channel>
</rss>
